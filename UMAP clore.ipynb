{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap.sparse_nndescent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1b65a7674fa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_nndescent\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_nn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m from umap.utils import (\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'umap.sparse_nndescent'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import locale\n",
    "from warnings import warn\n",
    "import time\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import check_random_state, check_array\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "except ImportError:\n",
    "    # sklearn.externals.joblib is deprecated in 0.21, will be removed in 0.23\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.sparse.csgraph\n",
    "import numba\n",
    "\n",
    "import umap.distances as dist\n",
    "\n",
    "import umap.sparse as sparse\n",
    "import umap.sparse_nndescent as sparse_nn\n",
    "\n",
    "from umap.utils import (\n",
    "    tau_rand_int,\n",
    "    deheap_sort,\n",
    "    submatrix,\n",
    "    ts,\n",
    "    csr_unique,\n",
    "    fast_knn_indices,\n",
    ")\n",
    "from umap.rp_tree import rptree_leaf_array, make_forest\n",
    "from umap.nndescent import (\n",
    "    # make_nn_descent,\n",
    "    # make_initialisations,\n",
    "    # make_initialized_nnd_search,\n",
    "    nn_descent,\n",
    "    initialized_nnd_search,\n",
    "    initialise_search,\n",
    ")\n",
    "from umap.rp_tree import rptree_leaf_array, make_forest\n",
    "from umap.spectral import spectral_layout\n",
    "from umap.utils import deheap_sort, submatrix\n",
    "from umap.layouts import (\n",
    "    optimize_layout_euclidean,\n",
    "    optimize_layout_generic,\n",
    "    optimize_layout_inverse,\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Use pynndescent, if installed (python 3 only)\n",
    "    from pynndescent import NNDescent\n",
    "    from pynndescent.distances import named_distances as pynn_named_distances\n",
    "    from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances\n",
    "\n",
    "    _HAVE_PYNNDESCENT = True\n",
    "except ImportError:\n",
    "    _HAVE_PYNNDESCENT = False\n",
    "\n",
    "locale.setlocale(locale.LC_NUMERIC, \"C\")\n",
    "\n",
    "INT32_MIN = np.iinfo(np.int32).min + 1\n",
    "INT32_MAX = np.iinfo(np.int32).max - 1\n",
    "\n",
    "SMOOTH_K_TOLERANCE = 1e-5\n",
    "MIN_K_DIST_SCALE = 1e-3\n",
    "NPY_INFINITY = np.inf\n",
    "\n",
    "\n",
    "def breadth_first_search(adjmat, start, min_vertices):\n",
    "    explored = []\n",
    "    queue = [start]\n",
    "    levels = {}\n",
    "    levels[start] = 0\n",
    "    max_level = np.inf\n",
    "    visited = [start]\n",
    "\n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        explored.append(node)\n",
    "        if max_level == np.inf and len(explored) > min_vertices:\n",
    "            max_level = max(levels.values())\n",
    "\n",
    "        if levels[node] + 1 < max_level:\n",
    "            neighbors = adjmat[node].indices\n",
    "            for neighbour in neighbors:\n",
    "                if neighbour not in visited:\n",
    "                    queue.append(neighbour)\n",
    "                    visited.append(neighbour)\n",
    "\n",
    "                    levels[neighbour] = levels[node] + 1\n",
    "\n",
    "    return np.array(explored)\n",
    "\n",
    "\n",
    "@numba.njit(\n",
    "    locals={\n",
    "        \"psum\": numba.types.float32,\n",
    "        \"lo\": numba.types.float32,\n",
    "        \"mid\": numba.types.float32,\n",
    "        \"hi\": numba.types.float32,\n",
    "    },\n",
    "    fastmath=True,\n",
    ")  # benchmarking `parallel=True` shows it to *decrease* performance\n",
    "def smooth_knn_dist(distances, k, n_iter=64, local_connectivity=1.0, bandwidth=1.0):\n",
    "    \"\"\"计算到第k个最近点的距离的连续版本。邻居。也就是说，这类似于KNN距离，但允许连续。\n",
    "取值，而不需要积分k。本质上，我们只是简单地。计算距离，使得我们生成的模糊集的基数。\n",
    "是k吗.\n",
    "    Parameters\n",
    "    ----------\n",
    "    distances: array of shape (n_samples, n_neighbors)\n",
    "        Distances to nearest neighbors for each samples. Each row should be a\n",
    "        sorted list of distances to a given samples nearest neighbors.\n",
    "    k: float\n",
    "        The number of nearest neighbors to approximate for.\n",
    "    n_iter: int (optional, default 64)\n",
    "        We need to binary search for the correct distance value. This is the\n",
    "        max number of iterations to use in such a search.\n",
    "    local_connectivity: int (optional, default 1)\n",
    "        The local connectivity required -- i.e. the number of nearest\n",
    "        neighbors that should be assumed to be connected at a local level.\n",
    "        The higher this value the more connected the manifold becomes\n",
    "        locally. In practice this should be not more than the local intrinsic\n",
    "        dimension of the manifold.\n",
    "    bandwidth: float (optional, default 1)\n",
    "        The target bandwidth of the kernel, larger values will produce\n",
    "        larger return values.\n",
    "    Returns\n",
    "    -------\n",
    "    knn_dist: array of shape (n_samples,)\n",
    "        The distance to kth nearest neighbor, as suitably approximated.\n",
    "    nn_dist: array of shape (n_samples,)\n",
    "        The distance to the 1st nearest neighbor for each point.\n",
    "    \"\"\"\n",
    "    target = np.log2(k) * bandwidth\n",
    "    rho = np.zeros(distances.shape[0], dtype=np.float32)\n",
    "    result = np.zeros(distances.shape[0], dtype=np.float32)\n",
    "\n",
    "    mean_distances = np.mean(distances)\n",
    "\n",
    "    for i in range(distances.shape[0]):\n",
    "        lo = 0.0\n",
    "        hi = NPY_INFINITY\n",
    "        mid = 1.0\n",
    "\n",
    "        # TODO: This is very inefficient, but will do for now. FIXME\n",
    "        ith_distances = distances[i]\n",
    "        non_zero_dists = ith_distances[ith_distances > 0.0]\n",
    "        if non_zero_dists.shape[0] >= local_connectivity:\n",
    "            index = int(np.floor(local_connectivity))\n",
    "            interpolation = local_connectivity - index\n",
    "            if index > 0:\n",
    "                rho[i] = non_zero_dists[index - 1]\n",
    "                if interpolation > SMOOTH_K_TOLERANCE:\n",
    "                    rho[i] += interpolation * (\n",
    "                        non_zero_dists[index] - non_zero_dists[index - 1]\n",
    "                    )\n",
    "            else:\n",
    "                rho[i] = interpolation * non_zero_dists[0]\n",
    "        elif non_zero_dists.shape[0] > 0:\n",
    "            rho[i] = np.max(non_zero_dists)\n",
    "\n",
    "        for n in range(n_iter):\n",
    "           psum = 0.0\n",
    "            for j in range(1, distances.shape[1]):\n",
    "                d = distances[i, j] - rho[i]\n",
    "                if d > 0:\n",
    "                    psum += np.exp(-(d / mid))\n",
    "                else:\n",
    "                    psum += 1.0\n",
    "\n",
    "            if np.fabs(psum - target) < SMOOTH_K_TOLERANCE:\n",
    "                break\n",
    "\n",
    "            if psum > target:\n",
    "                hi = mid\n",
    "                mid = (lo + hi) / 2.0\n",
    "            else:\n",
    "                lo = mid\n",
    "                if hi == NPY_INFINITY:\n",
    "                    mid *= 2\n",
    "                else:\n",
    "                    mid = (lo + hi) / 2.0\n",
    "\n",
    "        result[i] = mid\n",
    "\n",
    "        # TODO: This is very inefficient, but will do for now. FIXME\n",
    "        if rho[i] > 0.0:\n",
    "            mean_ith_distances = np.mean(ith_distances)\n",
    "            if result[i] < MIN_K_DIST_SCALE * mean_ith_distances:\n",
    "                result[i] = MIN_K_DIST_SCALE * mean_ith_distances\n",
    "        else:\n",
    "            if result[i] < MIN_K_DIST_SCALE * mean_distances:\n",
    "                result[i] = MIN_K_DIST_SCALE * mean_distances\n",
    "\n",
    "    return result, rho\n",
    "\n",
    "\n",
    "def nearest_neighbors(\n",
    "    X,\n",
    "    n_neighbors,\n",
    "    metric,\n",
    "    metric_kwds,\n",
    "    angular,\n",
    "    random_state,\n",
    "    low_memory=False,\n",
    "    use_pynndescent=True,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Compute the ``n_neighbors`` nearest points for each data point in ``X``\n",
    "    under ``metric``. This may be exact, but more likely is approximated via\n",
    "    nearest neighbor descent.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array of shape (n_samples, n_features)\n",
    "        The input data to compute the k-neighbor graph of.\n",
    "    n_neighbors: int\n",
    "        The number of nearest neighbors to compute for each sample in ``X``.\n",
    "    metric: string or callable\n",
    "        The metric to use for the computation.\n",
    "    metric_kwds: dict\n",
    "        Any arguments to pass to the metric computation function.\n",
    "    angular: bool\n",
    "        Whether to use angular rp trees in NN approximation.\n",
    "    random_state: np.random state\n",
    "        The random state to use for approximate NN computations.\n",
    "    low_memory: bool (optional, default False)\n",
    "        Whether to pursue lower memory NNdescent.\n",
    "    verbose: bool (optional, default False)\n",
    "        Whether to print status data during the computation.\n",
    "    Returns\n",
    "    -------\n",
    "    knn_indices: array of shape (n_samples, n_neighbors)\n",
    "        The indices on the ``n_neighbors`` closest points in the dataset.\n",
    "    knn_dists: array of shape (n_samples, n_neighbors)\n",
    "        The distances to the ``n_neighbors`` closest points in the dataset.\n",
    "    rp_forest: list of trees\n",
    "        The random projection forest used for searching (if used, None otherwise)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(ts(), \"Finding Nearest Neighbors\")\n",
    "\n",
    "    if metric == \"precomputed\":\n",
    "        # Note that this does not support sparse distance matrices yet ...\n",
    "        # Compute indices of n nearest neighbors\n",
    "        knn_indices = fast_knn_indices(X, n_neighbors)\n",
    "        # knn_indices = np.argsort(X)[:, :n_neighbors]\n",
    "        # Compute the nearest neighbor distances\n",
    "        #   (equivalent to np.sort(X)[:,:n_neighbors])\n",
    "        knn_dists = X[np.arange(X.shape[0])[:, None], knn_indices].copy()\n",
    "\n",
    "        rp_forest = []\n",
    "    else:\n",
    "        # TODO: Hacked values for now\n",
    "        n_trees = 5 + int(round((X.shape[0]) ** 0.5 / 20.0))\n",
    "        n_iters = max(5, int(round(np.log2(X.shape[0]))))\n",
    "\n",
    "        if _HAVE_PYNNDESCENT and use_pynndescent:\n",
    "            nnd = NNDescent(\n",
    "                X,\n",
    "                n_neighbors=n_neighbors,\n",
    "                metric=metric,\n",
    "                metric_kwds=metric_kwds,\n",
    "                random_state=random_state,\n",
    "                n_trees=n_trees,\n",
    "                n_iters=n_iters,\n",
    "                max_candidates=60,\n",
    "                low_memory=low_memory,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            knn_indices, knn_dists = nnd.neighbor_graph\n",
    "            rp_forest = nnd\n",
    "        else:\n",
    "            # Otherwise fall back to nn descent in umap\n",
    "            if callable(metric):\n",
    "                _distance_func = metric\n",
    "            elif metric in dist.named_distances:\n",
    "                _distance_func = dist.named_distances[metric]\n",
    "            else:\n",
    "                raise ValueError(\"Metric is neither callable, nor a recognised string\")\n",
    "\n",
    "            rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n",
    "\n",
    "            if scipy.sparse.isspmatrix_csr(X):\n",
    "                if callable(metric):\n",
    "                    _distance_func = metric\n",
    "                else:\n",
    "                    try:\n",
    "                        _distance_func = sparse.sparse_named_distances[metric]\n",
    "                        if metric in sparse.sparse_need_n_features:\n",
    "                            metric_kwds[\"n_features\"] = X.shape[1]\n",
    "                    except KeyError as e:\n",
    "                        raise ValueError(\n",
    "                            \"Metric {} not supported for sparse data\".format(metric)\n",
    "                        ) from e\n",
    "\n",
    "                # Create a partial function for distances with arguments\n",
    "                if len(metric_kwds) > 0:\n",
    "                    dist_args = tuple(metric_kwds.values())\n",
    "\n",
    "                    @numba.njit()\n",
    "                    def _partial_dist_func(ind1, data1, ind2, data2):\n",
    "                        return _distance_func(ind1, data1, ind2, data2, *dist_args)\n",
    "\n",
    "                    distance_func = _partial_dist_func\n",
    "                else:\n",
    "                    distance_func = _distance_func\n",
    "                # metric_nn_descent = sparse.make_sparse_nn_descent(\n",
    "                #     distance_func, tuple(metric_kwds.values())\n",
    "                # )\n",
    "\n",
    "                if verbose:\n",
    "                    print(ts(), \"Building RP forest with\", str(n_trees), \"trees\")\n",
    "\n",
    "                rp_forest = make_forest(X, n_neighbors, n_trees, rng_state, angular)\n",
    "                leaf_array = rptree_leaf_array(rp_forest)\n",
    "\n",
    "                if verbose:\n",
    "                    print(ts(), \"NN descent for\", str(n_iters), \"iterations\")\n",
    "                knn_indices, knn_dists = sparse_nn.sparse_nn_descent(\n",
    "                    X.indices,\n",
    "                    X.indptr,\n",
    "                    X.data,\n",
    "                    X.shape[0],\n",
    "                    n_neighbors,\n",
    "                    rng_state,\n",
    "                    max_candidates=60,\n",
    "                    sparse_dist=distance_func,\n",
    "                    low_memory=low_memory,\n",
    "                    rp_tree_init=True,\n",
    "                    leaf_array=leaf_array,\n",
    "                    n_iters=n_iters,\n",
    "                    verbose=verbose,\n",
    "                )\n",
    "            else:\n",
    "                # metric_nn_descent = make_nn_descent(\n",
    "                #     distance_func, tuple(metric_kwds.values())\n",
    "                # )\n",
    "                if len(metric_kwds) > 0:\n",
    "                    dist_args = tuple(metric_kwds.values())\n",
    "\n",
    "                    @numba.njit()\n",
    "                    def _partial_dist_func(x, y):\n",
    "                        return _distance_func(x, y, *dist_args)\n",
    "\n",
    "                    distance_func = _partial_dist_func\n",
    "                else:\n",
    "                    distance_func = _distance_func\n",
    "\n",
    "                if verbose:\n",
    "                    print(ts(), \"Building RP forest with\", str(n_trees), \"trees\")\n",
    "                rp_forest = make_forest(X, n_neighbors, n_trees, rng_state, angular)\n",
    "                leaf_array = rptree_leaf_array(rp_forest)\n",
    "                if verbose:\n",
    "                    print(ts(), \"NN descent for\", str(n_iters), \"iterations\")\n",
    "                knn_indices, knn_dists = nn_descent(\n",
    "                    X,\n",
    "                    n_neighbors,\n",
    "                    rng_state,\n",
    "                    max_candidates=60,\n",
    "                    dist=distance_func,\n",
    "                    low_memory=low_memory,\n",
    "                    rp_tree_init=True,\n",
    "                    leaf_array=leaf_array,\n",
    "                    n_iters=n_iters,\n",
    "                    verbose=verbose,\n",
    "                )\n",
    "\n",
    "            if np.any(knn_indices < 0):\n",
    "                warn(\n",
    "                    \"Failed to correctly find n_neighbors for some samples.\"\n",
    "                    \"Results may be less than ideal. Try re-running with\"\n",
    "                    \"different parameters.\"\n",
    "                )\n",
    "    if verbose:\n",
    "        print(ts(), \"Finished Nearest Neighbor Search\")\n",
    "    return knn_indices, knn_dists, rp_forest\n",
    "\n",
    "\n",
    "@numba.njit(\n",
    "    locals={\n",
    "        \"knn_dists\": numba.types.float32[:, ::1],\n",
    "        \"sigmas\": numba.types.float32[::1],\n",
    "        \"rhos\": numba.types.float32[::1],\n",
    "        \"val\": numba.types.float32,\n",
    "    },\n",
    "    parallel=True,\n",
    "    fastmath=True,\n",
    ")\n",
    "def compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos):\n",
    "    \"\"\"构建每个局部的1-骨架的成员资格强度数据。模糊单纯集--这是一个稀疏矩阵，其中每行。\n",
    "一个局部模糊单纯集，它的隶属度是。1-到每个其他数据点的单工.\n",
    "    Parameters\n",
    "    ----------\n",
    "    knn_indices: array of shape (n_samples, n_neighbors)\n",
    "        The indices on the ``n_neighbors`` closest points in the dataset.\n",
    "    knn_dists: array of shape (n_samples, n_neighbors)\n",
    "        The distances to the ``n_neighbors`` closest points in the dataset.\n",
    "    sigmas: array of shape(n_samples)\n",
    "        The normalization factor derived from the metric tensor approximation.\n",
    "    rhos: array of shape(n_samples)\n",
    "        The local connectivity adjustment.\n",
    "    Returns\n",
    "    -------\n",
    "    rows: array of shape (n_samples * n_neighbors)\n",
    "        Row data for the resulting sparse matrix (coo format)\n",
    "    cols: array of shape (n_samples * n_neighbors)\n",
    "        Column data for the resulting sparse matrix (coo format)\n",
    "    vals: array of shape (n_samples * n_neighbors)\n",
    "        Entries for the resulting sparse matrix (coo format)\n",
    "    \"\"\"\n",
    "    n_samples = knn_indices.shape[0]\n",
    "    n_neighbors = knn_indices.shape[1]\n",
    "\n",
    "    rows = np.zeros(knn_indices.size, dtype=np.int32)\n",
    "    cols = np.zeros(knn_indices.size, dtype=np.int32)\n",
    "    vals = np.zeros(knn_indices.size, dtype=np.float32)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_neighbors):\n",
    "            if knn_indices[i, j] == -1:\n",
    "                continue  # We didn't get the full knn for i\n",
    "            if knn_indices[i, j] == i:\n",
    "                val = 0.0\n",
    "            elif knn_dists[i, j] - rhos[i] <= 0.0 or sigmas[i] == 0.0:\n",
    "                val = 1.0\n",
    "            else:\n",
    "                val = np.exp(-((knn_dists[i, j] - rhos[i]) / (sigmas[i])))\n",
    "\n",
    "            rows[i * n_neighbors + j] = i\n",
    "            cols[i * n_neighbors + j] = knn_indices[i, j]\n",
    "            vals[i * n_neighbors + j] = val\n",
    "\n",
    "    return rows, cols, vals\n",
    "\n",
    "\n",
    "def fuzzy_simplicial_set(\n",
    "    X,\n",
    "    n_neighbors,\n",
    "    random_state,\n",
    "    metric,\n",
    "    metric_kwds={},\n",
    "    knn_indices=None,\n",
    "    knn_dists=None,\n",
    "    angular=False,\n",
    "    set_op_mix_ratio=1.0,\n",
    "    local_connectivity=1.0,\n",
    "    apply_set_operations=True,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Given a set of data X, a neighborhood size, and a measure of distance\n",
    "    compute the fuzzy simplicial set (here represented as a fuzzy graph in\n",
    "    the form of a sparse matrix) associated to the data. This is done by\n",
    "    locally approximating geodesic distance at each point, creating a fuzzy\n",
    "    simplicial set for each such point, and then combining all the local\n",
    "    fuzzy simplicial sets into a global one via a fuzzy union.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array of shape (n_samples, n_features)\n",
    "        The data to be modelled as a fuzzy simplicial set.\n",
    "    n_neighbors: int\n",
    "        The number of neighbors to use to approximate geodesic distance.\n",
    "        Larger numbers induce more global estimates of the manifold that can\n",
    "        miss finer detail, while smaller values will focus on fine manifold\n",
    "        structure to the detriment of the larger picture.\n",
    "    random_state: numpy RandomState or equivalent\n",
    "        A state capable being used as a numpy random state.\n",
    "    metric: string or function (optional, default 'euclidean')\n",
    "        The metric to use to compute distances in high dimensional space.\n",
    "        If a string is passed it must match a valid predefined metric. If\n",
    "        a general metric is required a function that takes two 1d arrays and\n",
    "        returns a float can be provided. For performance purposes it is\n",
    "        required that this be a numba jit'd function. Valid string metrics\n",
    "        include:\n",
    "            * euclidean (or l2)\n",
    "            * manhattan (or l1)\n",
    "            * cityblock\n",
    "            * braycurtis\n",
    "            * canberra\n",
    "            * chebyshev\n",
    "            * correlation\n",
    "            * cosine\n",
    "            * dice\n",
    "            * hamming\n",
    "            * jaccard\n",
    "            * kulsinski\n",
    "            * ll_dirichlet\n",
    "            * mahalanobis\n",
    "            * matching\n",
    "            * minkowski\n",
    "            * rogerstanimoto\n",
    "            * russellrao\n",
    "            * seuclidean\n",
    "            * sokalmichener\n",
    "            * sokalsneath\n",
    "            * sqeuclidean\n",
    "            * yule\n",
    "            * wminkowski\n",
    "        Metrics that take arguments (such as minkowski, mahalanobis etc.)\n",
    "        can have arguments passed via the metric_kwds dictionary. At this\n",
    "        time care must be taken and dictionary elements must be ordered\n",
    "        appropriately; this will hopefully be fixed in the future.\n",
    "    metric_kwds: dict (optional, default {})\n",
    "        Arguments to pass on to the metric, such as the ``p`` value for\n",
    "        Minkowski distance.\n",
    "    knn_indices: array of shape (n_samples, n_neighbors) (optional)\n",
    "        If the k-nearest neighbors of each point has already been calculated\n",
    "        you can pass them in here to save computation time. This should be\n",
    "        an array with the indices of the k-nearest neighbors as a row for\n",
    "        each data point.\n",
    "    knn_dists: array of shape (n_samples, n_neighbors) (optional)\n",
    "        If the k-nearest neighbors of each point has already been calculated\n",
    "        you can pass them in here to save computation time. This should be\n",
    "        an array with the distances of the k-nearest neighbors as a row for\n",
    "        each data point.\n",
    "    angular: bool (optional, default False)\n",
    "        Whether to use angular/cosine distance for the random projection\n",
    "        forest for seeding NN-descent to determine approximate nearest\n",
    "        neighbors.\n",
    "    set_op_mix_ratio: float (optional, default 1.0)\n",
    "        Interpolate between (fuzzy) union and intersection as the set operation\n",
    "        used to combine local fuzzy simplicial sets to obtain a global fuzzy\n",
    "        simplicial sets. Both fuzzy set operations use the product t-norm.\n",
    "        The value of this parameter should be between 0.0 and 1.0; a value of\n",
    "        1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy\n",
    "        intersection.\n",
    "    local_connectivity: int (optional, default 1)\n",
    "        The local connectivity required -- i.e. the number of nearest\n",
    "        neighbors that should be assumed to be connected at a local level.\n",
    "        The higher this value the more connected the manifold becomes\n",
    "        locally. In practice this should be not more than the local intrinsic\n",
    "        dimension of the manifold.\n",
    "    verbose: bool (optional, default False)\n",
    "        Whether to report information on the current progress of the algorithm.\n",
    "    Returns\n",
    "    -------\n",
    "    fuzzy_simplicial_set: coo_matrix\n",
    "        A fuzzy simplicial set represented as a sparse matrix. The (i,\n",
    "        j) entry of the matrix represents the membership strength of the\n",
    "        1-simplex between the ith and jth sample points.\n",
    "    \"\"\"\n",
    "    if knn_indices is None or knn_dists is None:\n",
    "        knn_indices, knn_dists, _ = nearest_neighbors(\n",
    "            X, n_neighbors, metric, metric_kwds, angular, random_state, verbose=verbose\n",
    "        )\n",
    "\n",
    "    knn_dists = knn_dists.astype(np.float32)\n",
    "\n",
    "    sigmas, rhos = smooth_knn_dist(\n",
    "        knn_dists, float(n_neighbors), local_connectivity=float(local_connectivity),\n",
    "    )\n",
    "\n",
    "    rows, cols, vals = compute_membership_strengths(\n",
    "        knn_indices, knn_dists, sigmas, rhos\n",
    "    )\n",
    "\n",
    "    result = scipy.sparse.coo_matrix(\n",
    "        (vals, (rows, cols)), shape=(X.shape[0], X.shape[0])\n",
    "    )\n",
    "    result.eliminate_zeros()\n",
    "\n",
    "    if apply_set_operations:\n",
    "        transpose = result.transpose()\n",
    "\n",
    "        prod_matrix = result.multiply(transpose)\n",
    "\n",
    "        result = (\n",
    "            set_op_mix_ratio * (result + transpose - prod_matrix)\n",
    "            + (1.0 - set_op_mix_ratio) * prod_matrix\n",
    "        )\n",
    "\n",
    "    result.eliminate_zeros()\n",
    "\n",
    "    return result, sigmas, rhos\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def fast_intersection(rows, cols, values, target, unknown_dist=1.0, far_dist=5.0):\n",
    "    \"\"\"在相交单纯集的范畴距离假设下，进行快速相交\n",
    "    Parameters\n",
    "    ----------\n",
    "    rows: array\n",
    "        An array of the row of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    cols: array\n",
    "        An array of the column of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    values: array\n",
    "        An array of the value of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    target: array of shape (n_samples)\n",
    "        The categorical labels to use in the intersection.\n",
    "    unknown_dist: float (optional, default 1.0)\n",
    "        The distance an unknown label (-1) is assumed to be from any point.\n",
    "    far_dist float (optional, default 5.0)\n",
    "        The distance between unmatched labels.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for nz in range(rows.shape[0]):\n",
    "        i = rows[nz]\n",
    "        j = cols[nz]\n",
    "        if (target[i] == -1) or (target[j] == -1):\n",
    "            values[nz] *= np.exp(-unknown_dist)\n",
    "        elif target[i] != target[j]:\n",
    "            values[nz] *= np.exp(-far_dist)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def fast_metric_intersection(\n",
    "    rows, cols, values, discrete_space, metric, metric_args, scale\n",
    "):\n",
    "    \"\"\"Under the assumption of categorical distance for the intersecting\n",
    "    simplicial set perform a fast intersection.\n",
    "    Parameters\n",
    "    ----------\n",
    "    rows: array\n",
    "        An array of the row of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    cols: array\n",
    "        An array of the column of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    values: array of shape\n",
    "        An array of the values of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    discrete_space: array of shape (n_samples, n_features)\n",
    "        The vectors of categorical labels to use in the intersection.\n",
    "    metric: numba function\n",
    "        The function used to calculate distance over the target array.\n",
    "    scale: float\n",
    "        A scaling to apply to the metric.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for nz in range(rows.shape[0]):\n",
    "        i = rows[nz]\n",
    "        j = cols[nz]\n",
    "        dist = metric(discrete_space[i], discrete_space[j], *metric_args)\n",
    "        values[nz] *= np.exp(-(scale * dist))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def reprocess_row(probabilities, k=15, n_iters=32):\n",
    "    target = np.log2(k)\n",
    "\n",
    "    lo = 0.0\n",
    "    hi = NPY_INFINITY\n",
    "    mid = 1.0\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        psum = 0.0\n",
    "        for j in range(probabilities.shape[0]):\n",
    "            psum += pow(probabilities[j], mid)\n",
    "\n",
    "        if np.fabs(psum - target) < SMOOTH_K_TOLERANCE:\n",
    "            break\n",
    "\n",
    "        if psum < target:\n",
    "            hi = mid\n",
    "            mid = (lo + hi) / 2.0\n",
    "        else:\n",
    "            lo = mid\n",
    "            if hi == NPY_INFINITY:\n",
    "                mid *= 2\n",
    "            else:\n",
    "                mid = (lo + hi) / 2.0\n",
    "\n",
    "    return np.power(probabilities, mid)\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def reset_local_metrics(simplicial_set_indptr, simplicial_set_data):\n",
    "    for i in range(simplicial_set_indptr.shape[0] - 1):\n",
    "        simplicial_set_data[\n",
    "            simplicial_set_indptr[i] : simplicial_set_indptr[i + 1]\n",
    "        ] = reprocess_row(\n",
    "            simplicial_set_data[simplicial_set_indptr[i] : simplicial_set_indptr[i + 1]]\n",
    "        )\n",
    "    return\n",
    "\n",
    "\n",
    "def reset_local_connectivity(simplicial_set, reset_local_metric=False):\n",
    "    \"\"\"Reset the local connectivity requirement -- each data sample should\n",
    "    have complete confidence in at least one 1-simplex in the simplicial set.\n",
    "    We can enforce this by locally rescaling confidences, and then remerging the\n",
    "    different local simplicial sets together.\n",
    "    Parameters\n",
    "    ----------\n",
    "    simplicial_set: sparse matrix\n",
    "        The simplicial set for which to recalculate with respect to local\n",
    "        connectivity.\n",
    "    Returns\n",
    "    -------\n",
    "    simplicial_set: sparse_matrix\n",
    "        The recalculated simplicial set, now with the local connectivity\n",
    "        assumption restored.\n",
    "    \"\"\"\n",
    "    simplicial_set = normalize(simplicial_set, norm=\"max\")\n",
    "    if reset_local_metric:\n",
    "        simplicial_set = simplicial_set.tocsr()\n",
    "        reset_local_metrics(simplicial_set.indptr, simplicial_set.data)\n",
    "        simplicial_set = simplicial_set.tocoo()\n",
    "    transpose = simplicial_set.transpose()\n",
    "    prod_matrix = simplicial_set.multiply(transpose)\n",
    "    simplicial_set = simplicial_set + transpose - prod_matrix\n",
    "    simplicial_set.eliminate_zeros()\n",
    "\n",
    "    return simplicial_set\n",
    "\n",
    "\n",
    "def discrete_metric_simplicial_set_intersection(\n",
    "    simplicial_set,\n",
    "    discrete_space,\n",
    "    unknown_dist=1.0,\n",
    "    far_dist=5.0,\n",
    "    metric=None,\n",
    "    metric_kws={},\n",
    "    metric_scale=1.0,\n",
    "):\n",
    "    \"\"\"Combine a fuzzy simplicial set with another fuzzy simplicial set\n",
    "    generated from discrete metric data using discrete distances. The target\n",
    "    data is assumed to be categorical label data (a vector of labels),\n",
    "    and this will update the fuzzy simplicial set to respect that label data.\n",
    "    TODO: optional category cardinality based weighting of distance\n",
    "    Parameters\n",
    "    ----------\n",
    "    simplicial_set: sparse matrix\n",
    "        The input fuzzy simplicial set.\n",
    "    discrete_space: array of shape (n_samples)\n",
    "        The categorical labels to use in the intersection.\n",
    "    unknown_dist: float (optional, default 1.0)\n",
    "        The distance an unknown label (-1) is assumed to be from any point.\n",
    "    far_dist: float (optional, default 5.0)\n",
    "        The distance between unmatched labels.\n",
    "    metric: str (optional, default None)\n",
    "        If not None, then use this metric to determine the\n",
    "        distance between values.\n",
    "    metric_scale: float (optional, default 1.0)\n",
    "        If using a custom metric scale the distance values by\n",
    "        this value -- this controls the weighting of the\n",
    "        intersection. Larger values weight more toward target.\n",
    "    Returns\n",
    "    -------\n",
    "    simplicial_set: sparse matrix\n",
    "        The resulting intersected fuzzy simplicial set.\n",
    "    \"\"\"\n",
    "    simplicial_set = simplicial_set.tocoo()\n",
    "\n",
    "    if metric is not None:\n",
    "        # We presume target is now a 2d array, with each row being a\n",
    "        # vector of target info\n",
    "        if metric in dist.named_distances:\n",
    "            metric_func = dist.named_distances[metric]\n",
    "        else:\n",
    "            raise ValueError(\"Discrete intersection metric is not recognized\")\n",
    "\n",
    "        fast_metric_intersection(\n",
    "            simplicial_set.row,\n",
    "            simplicial_set.col,\n",
    "            simplicial_set.data,\n",
    "            discrete_space,\n",
    "            metric_func,\n",
    "            tuple(metric_kws.values()),\n",
    "            metric_scale,\n",
    "        )\n",
    "    else:\n",
    "        fast_intersection(\n",
    "            simplicial_set.row,\n",
    "            simplicial_set.col,\n",
    "            simplicial_set.data,\n",
    "            discrete_space,\n",
    "            unknown_dist,\n",
    "            far_dist,\n",
    "        )\n",
    "\n",
    "    simplicial_set.eliminate_zeros()\n",
    "\n",
    "    return reset_local_connectivity(simplicial_set)\n",
    "\n",
    "\n",
    "def general_simplicial_set_intersection(simplicial_set1, simplicial_set2, weight):\n",
    "\n",
    "    result = (simplicial_set1 + simplicial_set2).tocoo()\n",
    "    left = simplicial_set1.tocsr()\n",
    "    right = simplicial_set2.tocsr()\n",
    "\n",
    "    sparse.general_sset_intersection(\n",
    "        left.indptr,\n",
    "        left.indices,\n",
    "        left.data,\n",
    "        right.indptr,\n",
    "        right.indices,\n",
    "        right.data,\n",
    "        result.row,\n",
    "        result.col,\n",
    "        result.data,\n",
    "        weight,\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def make_epochs_per_sample(weights, n_epochs):\n",
    "    \"\"\"Given a set of weights and number of epochs generate the number of\n",
    "    epochs per sample for each weight.\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights: array of shape (n_1_simplices)\n",
    "        The weights ofhow much we wish to sample each 1-simplex.\n",
    "    n_epochs: int\n",
    "        The total number of epochs we want to train for.\n",
    "    Returns\n",
    "    -------\n",
    "    An array of number of epochs per sample, one for each 1-simplex.\n",
    "    \"\"\"\n",
    "    result = -1.0 * np.ones(weights.shape[0], dtype=np.float64)\n",
    "    n_samples = n_epochs * (weights / weights.max())\n",
    "    result[n_samples > 0] = float(n_epochs) / n_samples[n_samples > 0]\n",
    "    return result\n",
    "\n",
    "\n",
    "def simplicial_set_embedding(\n",
    "    data,\n",
    "    graph,\n",
    "    n_components,\n",
    "    initial_alpha,\n",
    "    a,\n",
    "    b,\n",
    "    gamma,\n",
    "    negative_sample_rate,\n",
    "    n_epochs,\n",
    "    init,\n",
    "    random_state,\n",
    "    metric,\n",
    "    metric_kwds,\n",
    "    output_metric=dist.named_distances_with_gradients[\"euclidean\"],\n",
    "    output_metric_kwds={},\n",
    "    euclidean_output=True,\n",
    "    parallel=False,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Perform a fuzzy simplicial set embedding, using a specified\n",
    "    initialisation method and then minimizing the fuzzy set cross entropy\n",
    "    between the 1-skeletons of the high and low dimensional fuzzy simplicial\n",
    "    sets.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: array of shape (n_samples, n_features)\n",
    "        The source data to be embedded by UMAP.\n",
    "    graph: sparse matrix\n",
    "        The 1-skeleton of the high dimensional fuzzy simplicial set as\n",
    "        represented by a graph for which we require a sparse matrix for the\n",
    "        (weighted) adjacency matrix.\n",
    "    n_components: int\n",
    "        The dimensionality of the euclidean space into which to embed the data.\n",
    "    initial_alpha: float\n",
    "        Initial learning rate for the SGD.\n",
    "    a: float\n",
    "        Parameter of differentiable approximation of right adjoint functor\n",
    "    b: float\n",
    "        Parameter of differentiable approximation of right adjoint functor\n",
    "    gamma: float\n",
    "        Weight to apply to negative samples.\n",
    "    negative_sample_rate: int (optional, default 5)\n",
    "        The number of negative samples to select per positive sample\n",
    "        in the optimization process. Increasing this value will result\n",
    "        in greater repulsive force being applied, greater optimization\n",
    "        cost, but slightly more accuracy.\n",
    "    n_epochs: int (optional, default 0)\n",
    "        The number of training epochs to be used in optimizing the\n",
    "        low dimensional embedding. Larger values result in more accurate\n",
    "        embeddings. If 0 is specified a value will be selected based on\n",
    "        the size of the input dataset (200 for large datasets, 500 for small).\n",
    "    init: string\n",
    "        How to initialize the low dimensional embedding. Options are:\n",
    "            * 'spectral': use a spectral embedding of the fuzzy 1-skeleton\n",
    "            * 'random': assign initial embedding positions at random.\n",
    "            * A numpy array of initial embedding positions.\n",
    "    random_state: numpy RandomState or equivalent\n",
    "        A state capable being used as a numpy random state.\n",
    "    metric: string or callable\n",
    "        The metric used to measure distance in high dimensional space; used if\n",
    "        multiple connected components need to be layed out.\n",
    "    metric_kwds: dict\n",
    "        Key word arguments to be passed to the metric function; used if\n",
    "        multiple connected components need to be layed out.\n",
    "    output_metric: function\n",
    "        Function returning the distance between two points in embedding space and\n",
    "        the gradient of the distance wrt the first argument.\n",
    "    output_metric_kwds: dict\n",
    "        Key word arguments to be passed to the output_metric function.\n",
    "    euclidean_output: bool\n",
    "        Whether to use the faster code specialised for euclidean output metrics\n",
    "    parallel: bool (optional, default False)\n",
    "        Whether to run the computation using numba parallel.\n",
    "        Running in parallel is non-deterministic, and is not used\n",
    "        if a random seed has been set, to ensure reproducibility.\n",
    "    verbose: bool (optional, default False)\n",
    "        Whether to report information on the current progress of the algorithm.\n",
    "    Returns\n",
    "    -------\n",
    "    embedding: array of shape (n_samples, n_components)\n",
    "        The optimized of ``graph`` into an ``n_components`` dimensional\n",
    "        euclidean space.\n",
    "    \"\"\"\n",
    "    graph = graph.tocoo()\n",
    "    graph.sum_duplicates()\n",
    "    n_vertices = graph.shape[1]\n",
    "\n",
    "    if n_epochs <= 0:\n",
    "        # For smaller datasets we can use more epochs\n",
    "        if graph.shape[0] <= 10000:\n",
    "            n_epochs = 500\n",
    "        else:\n",
    "            n_epochs = 200\n",
    "\n",
    "    graph.data[graph.data < (graph.data.max() / float(n_epochs))] = 0.0\n",
    "    graph.eliminate_zeros()\n",
    "\n",
    "    if isinstance(init, str) and init == \"random\":\n",
    "        embedding = random_state.uniform(\n",
    "            low=-10.0, high=10.0, size=(graph.shape[0], n_components)\n",
    "        ).astype(np.float32)\n",
    "    elif isinstance(init, str) and init == \"spectral\":\n",
    "        # We add a little noise to avoid local minima for optimization to come\n",
    "        initialisation = spectral_layout(\n",
    "            data,\n",
    "            graph,\n",
    "            n_components,\n",
    "            random_state,\n",
    "            metric=metric,\n",
    "            metric_kwds=metric_kwds,\n",
    "        )\n",
    "        expansion = 10.0 / np.abs(initialisation).max()\n",
    "        embedding = (initialisation * expansion).astype(\n",
    "            np.float32\n",
    "        ) + random_state.normal(\n",
    "            scale=0.0001, size=[graph.shape[0], n_components]\n",
    "        ).astype(\n",
    "            np.float32\n",
    "        )\n",
    "    else:\n",
    "        init_data = np.array(init)\n",
    "        if len(init_data.shape) == 2:\n",
    "            if np.unique(init_data, axis=0).shape[0] < init_data.shape[0]:\n",
    "                tree = KDTree(init_data)\n",
    "                dist, ind = tree.query(init_data, k=2)\n",
    "                nndist = np.mean(dist[:, 1])\n",
    "                embedding = init_data + random_state.normal(\n",
    "                    scale=0.001 * nndist, size=init_data.shape\n",
    "                ).astype(np.float32)\n",
    "            else:\n",
    "                embedding = init_data\n",
    "\n",
    "    epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n",
    "\n",
    "    head = graph.row\n",
    "    tail = graph.col\n",
    "    weight = graph.data\n",
    "\n",
    "    rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n",
    "\n",
    "    embedding = (\n",
    "        10.0\n",
    "        * (embedding - np.min(embedding, 0))\n",
    "        / (np.max(embedding, 0) - np.min(embedding, 0))\n",
    "    ).astype(np.float32, order=\"C\")\n",
    "\n",
    "    if euclidean_output:\n",
    "        embedding = optimize_layout_euclidean(\n",
    "            embedding,\n",
    "            embedding,\n",
    "            head,\n",
    "            tail,\n",
    "            n_epochs,\n",
    "            n_vertices,\n",
    "            epochs_per_sample,\n",
    "            a,\n",
    "            b,\n",
    "            rng_state,\n",
    "            gamma,\n",
    "            initial_alpha,\n",
    "            negative_sample_rate,\n",
    "            parallel=parallel,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    else:\n",
    "        embedding = optimize_layout_generic(\n",
    "            embedding,\n",
    "            embedding,\n",
    "            head,\n",
    "            tail,\n",
    "            n_epochs,\n",
    "            n_vertices,\n",
    "            epochs_per_sample,\n",
    "            a,\n",
    "            b,\n",
    "            rng_state,\n",
    "            gamma,\n",
    "            initial_alpha,\n",
    "            negative_sample_rate,\n",
    "            output_metric,\n",
    "            tuple(output_metric_kwds.values()),\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def init_transform(indices, weights, embedding):\n",
    "    \"\"\"Given indices and weights and an original embeddings\n",
    "    initialize the positions of new points relative to the\n",
    "    indices and weights (of their neighbors in the source data).\n",
    "    Parameters\n",
    "    ----------\n",
    "    indices: array of shape (n_new_samples, n_neighbors)\n",
    "        The indices of the neighbors of each new sample\n",
    "    weights: array of shape (n_new_samples, n_neighbors)\n",
    "        The membership strengths of associated 1-simplices\n",
    "        for each of the new samples.\n",
    "    embedding: array of shape (n_samples, dim)\n",
    "        The original embedding of the source data.\n",
    "    Returns\n",
    "    -------\n",
    "    new_embedding: array of shape (n_new_samples, dim)\n",
    "        An initial embedding of the new sample points.\n",
    "    \"\"\"\n",
    "    result = np.zeros((indices.shape[0], embedding.shape[1]), dtype=np.float32)\n",
    "\n",
    "    for i in range(indices.shape[0]):\n",
    "        for j in range(indices.shape[1]):\n",
    "            for d in range(embedding.shape[1]):\n",
    "                result[i, d] += weights[i, j] * embedding[indices[i, j], d]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def find_ab_params(spread, min_dist):\n",
    "    \"\"\"Fit a, b params for the differentiable curve used in lower\n",
    "    dimensional fuzzy simplicial complex construction. We want the\n",
    "    smooth curve (from a pre-defined family with simple gradient) that\n",
    "    best matches an offset exponential decay.\n",
    "    \"\"\"\n",
    "\n",
    "    def curve(x, a, b):\n",
    "        return 1.0 / (1.0 + a * x ** (2 * b))\n",
    "\n",
    "    xv = np.linspace(0, spread * 3, 300)\n",
    "    yv = np.zeros(xv.shape)\n",
    "    yv[xv < min_dist] = 1.0\n",
    "    yv[xv >= min_dist] = np.exp(-(xv[xv >= min_dist] - min_dist) / spread)\n",
    "    params, covar = curve_fit(curve, xv, yv)\n",
    "    return params[0], params[1]\n",
    "\n",
    "\n",
    "class UMAP(BaseEstimator):\n",
    "    \"\"\"Uniform Manifold Approximation and Projection\n",
    "    Finds a low dimensional embedding of the data that approximates\n",
    "    an underlying manifold.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_neighbors: float (optional, default 15)\n",
    "        The size of local neighborhood (in terms of number of neighboring\n",
    "        sample points) used for manifold approximation. Larger values\n",
    "        result in more global views of the manifold, while smaller\n",
    "        values result in more local data being preserved. In general\n",
    "        values should be in the range 2 to 100.\n",
    "    n_components: int (optional, default 2)\n",
    "        The dimension of the space to embed into. This defaults to 2 to\n",
    "        provide easy visualization, but can reasonably be set to any\n",
    "        integer value in the range 2 to 100.\n",
    "    metric: string or function (optional, default 'euclidean')\n",
    "        The metric to use to compute distances in high dimensional space.\n",
    "        If a string is passed it must match a valid predefined metric. If\n",
    "        a general metric is required a function that takes two 1d arrays and\n",
    "        returns a float can be provided. For performance purposes it is\n",
    "        required that this be a numba jit'd function. Valid string metrics\n",
    "        include:\n",
    "            * euclidean\n",
    "            * manhattan\n",
    "            * chebyshev\n",
    "            * minkowski\n",
    "            * canberra\n",
    "            * braycurtis\n",
    "            * mahalanobis\n",
    "            * wminkowski\n",
    "            * seuclidean\n",
    "            * cosine\n",
    "            * correlation\n",
    "            * haversine\n",
    "            * hamming\n",
    "            * jaccard\n",
    "            * dice\n",
    "            * russelrao\n",
    "            * kulsinski\n",
    "            * ll_dirichlet\n",
    "            * hellinger\n",
    "            * rogerstanimoto\n",
    "            * sokalmichener\n",
    "            * sokalsneath\n",
    "            * yule\n",
    "        Metrics that take arguments (such as minkowski, mahalanobis etc.)\n",
    "        can have arguments passed via the metric_kwds dictionary. At this\n",
    "        time care must be taken and dictionary elements must be ordered\n",
    "        appropriately; this will hopefully be fixed in the future.\n",
    "    n_epochs: int (optional, default None)\n",
    "        The number of training epochs to be used in optimizing the\n",
    "        low dimensional embedding. Larger values result in more accurate\n",
    "        embeddings. If None is specified a value will be selected based on\n",
    "        the size of the input dataset (200 for large datasets, 500 for small).\n",
    "    learning_rate: float (optional, default 1.0)\n",
    "        The initial learning rate for the embedding optimization.\n",
    "    init: string (optional, default 'spectral')\n",
    "        How to initialize the low dimensional embedding. Options are:\n",
    "            * 'spectral': use a spectral embedding of the fuzzy 1-skeleton\n",
    "            * 'random': assign initial embedding positions at random.\n",
    "            * A numpy array of initial embedding positions.\n",
    "    min_dist: float (optional, default 0.1)\n",
    "        The effective minimum distance between embedded points. Smaller values\n",
    "        will result in a more clustered/clumped embedding where nearby points\n",
    "        on the manifold are drawn closer together, while larger values will\n",
    "        result on a more even dispersal of points. The value should be set\n",
    "        relative to the ``spread`` value, which determines the scale at which\n",
    "        embedded points will be spread out.\n",
    "    spread: float (optional, default 1.0)\n",
    "        The effective scale of embedded points. In combination with ``min_dist``\n",
    "        this determines how clustered/clumped the embedded points are.\n",
    "    low_memory: bool (optional, default False)\n",
    "        For some datasets the nearest neighbor computation can consume a lot of\n",
    "        memory. If you find that UMAP is failing due to memory constraints\n",
    "        consider setting this option to True. This approach is more\n",
    "        computationally expensive, but avoids excessive memory use.\n",
    "    set_op_mix_ratio: float (optional, default 1.0)\n",
    "        Interpolate between (fuzzy) union and intersection as the set operation\n",
    "        used to combine local fuzzy simplicial sets to obtain a global fuzzy\n",
    "        simplicial sets. Both fuzzy set operations use the product t-norm.\n",
    "        The value of this parameter should be between 0.0 and 1.0; a value of\n",
    "        1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy\n",
    "        intersection.\n",
    "    local_connectivity: int (optional, default 1)\n",
    "        The local connectivity required -- i.e. the number of nearest\n",
    "        neighbors that should be assumed to be connected at a local level.\n",
    "        The higher this value the more connected the manifold becomes\n",
    "        locally. In practice this should be not more than the local intrinsic\n",
    "        dimension of the manifold.\n",
    "    repulsion_strength: float (optional, default 1.0)\n",
    "        Weighting applied to negative samples in low dimensional embedding\n",
    "        optimization. Values higher than one will result in greater weight\n",
    "        being given to negative samples.\n",
    "    negative_sample_rate: int (optional, default 5)\n",
    "        The number of negative samples to select per positive sample\n",
    "        in the optimization process. Increasing this value will result\n",
    "        in greater repulsive force being applied, greater optimization\n",
    "        cost, but slightly more accuracy.\n",
    "    transform_queue_size: float (optional, default 4.0)\n",
    "        For transform operations (embedding new points using a trained model_\n",
    "        this will control how aggressively to search for nearest neighbors.\n",
    "        Larger values will result in slower performance but more accurate\n",
    "        nearest neighbor evaluation.\n",
    "    a: float (optional, default None)\n",
    "        More specific parameters controlling the embedding. If None these\n",
    "        values are set automatically as determined by ``min_dist`` and\n",
    "        ``spread``.\n",
    "    b: float (optional, default None)\n",
    "        More specific parameters controlling the embedding. If None these\n",
    "        values are set automatically as determined by ``min_dist`` and\n",
    "        ``spread``.\n",
    "    random_state: int, RandomState instance or None, optional (default: None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    metric_kwds: dict (optional, default None)\n",
    "        Arguments to pass on to the metric, such as the ``p`` value for\n",
    "        Minkowski distance. If None then no arguments are passed on.\n",
    "    angular_rp_forest: bool (optional, default False)\n",
    "        Whether to use an angular random projection forest to initialise\n",
    "        the approximate nearest neighbor search. This can be faster, but is\n",
    "        mostly on useful for metric that use an angular style distance such\n",
    "        as cosine, correlation etc. In the case of those metrics angular forests\n",
    "        will be chosen automatically.\n",
    "    target_n_neighbors: int (optional, default -1)\n",
    "        The number of nearest neighbors to use to construct the target simplcial\n",
    "        set. If set to -1 use the ``n_neighbors`` value.\n",
    "    target_metric: string or callable (optional, default 'categorical')\n",
    "        The metric used to measure distance for a target array is using supervised\n",
    "        dimension reduction. By default this is 'categorical' which will measure\n",
    "        distance in terms of whether categories match or are different. Furthermore,\n",
    "        if semi-supervised is required target values of -1 will be trated as\n",
    "        unlabelled under the 'categorical' metric. If the target array takes\n",
    "        continuous values (e.g. for a regression problem) then metric of 'l1'\n",
    "        or 'l2' is probably more appropriate.\n",
    "    target_metric_kwds: dict (optional, default None)\n",
    "        Keyword argument to pass to the target metric when performing\n",
    "        supervised dimension reduction. If None then no arguments are passed on.\n",
    "    target_weight: float (optional, default 0.5)\n",
    "        weighting factor between data topology and target topology. A value of\n",
    "        0.0 weights entirely on data, a value of 1.0 weights entirely on target.\n",
    "        The default of 0.5 balances the weighting equally between data and target.\n",
    "    transform_seed: int (optional, default 42)\n",
    "        Random seed used for the stochastic aspects of the transform operation.\n",
    "        This ensures consistency in transform operations.\n",
    "    verbose: bool (optional, default False)\n",
    "        Controls verbosity of logging.\n",
    "    unique: bool (optional, default False)\n",
    "        Controls if the rows of your data should be uniqued before being\n",
    "        embedded.  If you have more duplicates than you have n_neighbour\n",
    "        you can have the identical data points lying in different regions of\n",
    "        your space.  It also violates the definition of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_neighbors=15,\n",
    "        n_components=2,\n",
    "        metric=\"euclidean\",\n",
    "        metric_kwds=None,\n",
    "        output_metric=\"euclidean\",\n",
    "        output_metric_kwds=None,\n",
    "        n_epochs=None,\n",
    "        learning_rate=1.0,\n",
    "        init=\"spectral\",\n",
    "        min_dist=0.1,\n",
    "        spread=1.0,\n",
    "        low_memory=False,\n",
    "        set_op_mix_ratio=1.0,\n",
    "        local_connectivity=1.0,\n",
    "        repulsion_strength=1.0,\n",
    "        negative_sample_rate=5,\n",
    "        transform_queue_size=4.0,\n",
    "        a=None,\n",
    "        b=None,\n",
    "        random_state=None,\n",
    "        angular_rp_forest=False,\n",
    "        target_n_neighbors=-1,\n",
    "        target_metric=\"categorical\",\n",
    "        target_metric_kwds=None,\n",
    "        target_weight=0.5,\n",
    "        transform_seed=42,\n",
    "        force_approximation_algorithm=False,\n",
    "        verbose=False,\n",
    "        unique=False,\n",
    "    ):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.output_metric = output_metric\n",
    "        self.target_metric = target_metric\n",
    "        self.metric_kwds = metric_kwds\n",
    "        self.output_metric_kwds = output_metric_kwds\n",
    "        self.n_epochs = n_epochs\n",
    "        self.init = init\n",
    "        self.n_components = n_components\n",
    "        self.repulsion_strength = repulsion_strength\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.spread = spread\n",
    "        self.min_dist = min_dist\n",
    "        self.low_memory = low_memory\n",
    "        self.set_op_mix_ratio = set_op_mix_ratio\n",
    "        self.local_connectivity = local_connectivity\n",
    "        self.negative_sample_rate = negative_sample_rate\n",
    "        self.random_state = random_state\n",
    "        self.angular_rp_forest = angular_rp_forest\n",
    "        self.transform_queue_size = transform_queue_size\n",
    "        self.target_n_neighbors = target_n_neighbors\n",
    "        self.target_metric = target_metric\n",
    "        self.target_metric_kwds = target_metric_kwds\n",
    "        self.target_weight = target_weight\n",
    "        self.transform_seed = transform_seed\n",
    "        self.force_approximation_algorithm = force_approximation_algorithm\n",
    "        self.verbose = verbose\n",
    "        self.unique = unique\n",
    "\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def _validate_parameters(self):\n",
    "        if self.set_op_mix_ratio < 0.0 or self.set_op_mix_ratio > 1.0:\n",
    "            raise ValueError(\"set_op_mix_ratio must be between 0.0 and 1.0\")\n",
    "        if self.repulsion_strength < 0.0:\n",
    "            raise ValueError(\"repulsion_strength cannot be negative\")\n",
    "        if self.min_dist > self.spread:\n",
    "            raise ValueError(\"min_dist must be less than or equal to spread\")\n",
    "        if self.min_dist < 0.0:\n",
    "            raise ValueError(\"min_dist cannot be negative\")\n",
    "        if not isinstance(self.init, str) and not isinstance(self.init, np.ndarray):\n",
    "            raise ValueError(\"init must be a string or ndarray\")\n",
    "        if isinstance(self.init, str) and self.init not in (\"spectral\", \"random\"):\n",
    "            raise ValueError('string init values must be \"spectral\" or \"random\"')\n",
    "        if (\n",
    "            isinstance(self.init, np.ndarray)\n",
    "            and self.init.shape[1] != self.n_components\n",
    "        ):\n",
    "            raise ValueError(\"init ndarray must match n_components value\")\n",
    "        if not isinstance(self.metric, str) and not callable(self.metric):\n",
    "            raise ValueError(\"metric must be string or callable\")\n",
    "        if self.negative_sample_rate < 0:\n",
    "            raise ValueError(\"negative sample rate must be positive\")\n",
    "        if self._initial_alpha < 0.0:\n",
    "            raise ValueError(\"learning_rate must be positive\")\n",
    "        if self.n_neighbors < 2:\n",
    "            raise ValueError(\"n_neighbors must be greater than 1\")\n",
    "        if self.target_n_neighbors < 2 and self.target_n_neighbors != -1:\n",
    "            raise ValueError(\"target_n_neighbors must be greater than 1\")\n",
    "        if not isinstance(self.n_components, int):\n",
    "            if isinstance(self.n_components, str):\n",
    "                raise ValueError(\"n_components must be an int\")\n",
    "            if self.n_components % 1 != 0:\n",
    "                raise ValueError(\"n_components must be a whole number\")\n",
    "            try:\n",
    "                # this will convert other types of int (eg. numpy int64)\n",
    "                # to Python int\n",
    "                self.n_components = int(self.n_components)\n",
    "            except ValueError:\n",
    "                raise ValueError(\"n_components must be an int\")\n",
    "        if self.n_components < 1:\n",
    "            raise ValueError(\"n_components must be greater than 0\")\n",
    "        if self.n_epochs is not None and (\n",
    "            self.n_epochs <= 10 or not isinstance(self.n_epochs, int)\n",
    "        ):\n",
    "            raise ValueError(\"n_epochs must be a positive integer of at least 10\")\n",
    "        if self.metric_kwds is None:\n",
    "            self._metric_kwds = {}\n",
    "        else:\n",
    "            self._metric_kwds = self.metric_kwds\n",
    "        if self.output_metric_kwds is None:\n",
    "            self._output_metric_kwds = {}\n",
    "        else:\n",
    "            self._output_metric_kwds = self.output_metric_kwds\n",
    "        if self.target_metric_kwds is None:\n",
    "            self._target_metric_kwds = {}\n",
    "        else:\n",
    "            self._target_metric_kwds = self.target_metric_kwds\n",
    "        # check sparsity of data upfront to set proper _input_distance_func &\n",
    "        # save repeated checks later on\n",
    "        if scipy.sparse.isspmatrix_csr(self._raw_data):\n",
    "            self._sparse_data = True\n",
    "        else:\n",
    "            self._sparse_data = False\n",
    "        # set input distance metric & inverse_transform distance metric\n",
    "        if callable(self.metric):\n",
    "            in_returns_grad = self._check_custom_metric(\n",
    "                self.metric, self._metric_kwds, self._raw_data\n",
    "            )\n",
    "            if in_returns_grad:\n",
    "                _m = self.metric\n",
    "\n",
    "                @numba.njit(fastmath=True)\n",
    "                def _dist_only(x, y, *kwds):\n",
    "                    return _m(x, y, *kwds)[0]\n",
    "\n",
    "                self._input_distance_func = _dist_only\n",
    "                self._inverse_distance_func = self.metric\n",
    "            else:\n",
    "                self._input_distance_func = self.metric\n",
    "                self._inverse_distance_func = None\n",
    "                warn(\n",
    "                    \"custom distance metric does not return gradient; inverse_transform will be unavailable. \"\n",
    "                    \"To enable using inverse_transform method method, define a distance function that returns \"\n",
    "                    \"a tuple of (distance [float], gradient [np.array])\"\n",
    "                )\n",
    "        elif self.metric == \"precomputed\":\n",
    "            if self.unique is False:\n",
    "                raise ValueError(\"unique is poorly defined on a precomputed metric\")\n",
    "            warn(\n",
    "                \"using precomputed metric; transform will be unavailable for new data and inverse_transform \"\n",
    "                \"will be unavailable for all data\"\n",
    "            )\n",
    "            self._input_distance_func = self.metric\n",
    "            self._inverse_distance_func = None\n",
    "        elif self.metric == \"hellinger\" and self._raw_data.min() < 0:\n",
    "            raise ValueError(\"Metric 'hellinger' does not support negative values\")\n",
    "        elif self.metric in dist.named_distances:\n",
    "            if self._sparse_data:\n",
    "                if self.metric in sparse.sparse_named_distances:\n",
    "                    self._input_distance_func = sparse.sparse_named_distances[\n",
    "                        self.metric\n",
    "                    ]\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Metric {} is not supported for sparse data\".format(self.metric)\n",
    "                    )\n",
    "            else:\n",
    "                self._input_distance_func = dist.named_distances[self.metric]\n",
    "            try:\n",
    "                self._inverse_distance_func = dist.named_distances_with_gradients[\n",
    "                    self.metric\n",
    "                ]\n",
    "            except KeyError:\n",
    "                warn(\n",
    "                    \"gradient function is not yet implemented for {} distance metric; \"\n",
    "                    \"inverse_transform will be unavailable\".format(self.metric)\n",
    "                )\n",
    "                self._inverse_distance_func = None\n",
    "        else:\n",
    "            raise ValueError(\"metric is neither callable nor a recognised string\")\n",
    "        # set ooutput distance metric\n",
    "        if callable(self.output_metric):\n",
    "            out_returns_grad = self._check_custom_metric(\n",
    "                self.output_metric, self._output_metric_kwds\n",
    "            )\n",
    "            if out_returns_grad:\n",
    "                self._output_distance_func = self.output_metric\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"custom output_metric must return a tuple of (distance [float], gradient [np.array])\"\n",
    "                )\n",
    "        elif self.output_metric == \"precomputed\":\n",
    "            raise ValueError(\"output_metric cannnot be 'precomputed'\")\n",
    "        elif self.output_metric in dist.named_distances_with_gradients:\n",
    "            self._output_distance_func = dist.named_distances_with_gradients[\n",
    "                self.output_metric\n",
    "            ]\n",
    "        elif self.output_metric in dist.named_distances:\n",
    "            raise ValueError(\n",
    "                \"gradient function is not yet implemented for {}.\".format(\n",
    "                    self.output_metric\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"output_metric is neither callable nor a recognised string\"\n",
    "            )\n",
    "        # set angularity for NN search based on metric\n",
    "        if self.metric in (\n",
    "            \"cosine\",\n",
    "            \"correlation\",\n",
    "            \"dice\",\n",
    "            \"jaccard\",\n",
    "            \"ll_dirichlet\",\n",
    "            \"hellinger\",\n",
    "        ):\n",
    "            self.angular_rp_forest = True\n",
    "\n",
    "    def _check_custom_metric(self, metric, kwds, data=None):\n",
    "        # quickly check to determine whether user-defined\n",
    "        # self.metric/self.output_metric returns both distance and gradient\n",
    "        if data is not None:\n",
    "            # if checking the high-dimensional distance metric, test directly on\n",
    "            # input data so we don't risk violating any assumptions potentially\n",
    "            # hard-coded in the metric (e.g., bounded; non-negative)\n",
    "            x, y = data[np.random.randint(0, data.shape[0], 2)]\n",
    "        else:\n",
    "            # if checking the manifold distance metric, simulate some data on a\n",
    "            # reasonable interval with output dimensionality\n",
    "            x, y = np.random.uniform(low=-10, high=10, size=(2, self.n_components))\n",
    "        metric_out = metric(x, y, **kwds)\n",
    "        # True if metric returns iterable of length 2, False otherwise\n",
    "        return hasattr(metric_out, \"__iter__\") and len(metric_out) == 2\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit X into an embedded space.\n",
    "        Optionally use y for supervised dimension reduction.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "            If the metric is 'precomputed' X must be a square distance\n",
    "            matrix. Otherwise it contains a sample per row. If the method\n",
    "            is 'exact', X may be a sparse matrix of type 'csr', 'csc'\n",
    "            or 'coo'.\n",
    "        y : array, shape (n_samples)\n",
    "            A target array for supervised dimension reduction. How this is\n",
    "            handled is determined by parameters UMAP was instantiated with.\n",
    "            The relevant attributes are ``target_metric`` and\n",
    "            ``target_metric_kwds``.\n",
    "        \"\"\"\n",
    "\n",
    "        X = check_array(X, dtype=np.float32, accept_sparse=\"csr\", order=\"C\")\n",
    "        self._raw_data = X\n",
    "\n",
    "        # Handle all the optional arguments, setting default\n",
    "        if self.a is None or self.b is None:\n",
    "            self._a, self._b = find_ab_params(self.spread, self.min_dist)\n",
    "        else:\n",
    "            self._a = self.a\n",
    "            self._b = self.b\n",
    "\n",
    "        if isinstance(self.init, np.ndarray):\n",
    "            init = check_array(self.init, dtype=np.float32, accept_sparse=False)\n",
    "        else:\n",
    "            init = self.init\n",
    "\n",
    "        self._initial_alpha = self.learning_rate\n",
    "\n",
    "        self._validate_parameters()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(str(self))\n",
    "\n",
    "        # Check if we should unique the data\n",
    "        # We've already ensured that we aren't in the precomputed case\n",
    "        if self.unique:\n",
    "            # check if the matrix is dense\n",
    "            if self._sparse_data:\n",
    "                # Call a sparse unique function\n",
    "                index, inverse, counts = csr_unique(X)\n",
    "            else:\n",
    "                index, inverse, counts = np.unique(\n",
    "                    X,\n",
    "                    return_index=True,\n",
    "                    return_inverse=True,\n",
    "                    return_counts=True,\n",
    "                    axis=0,\n",
    "                )[1:4]\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    \"Unique=True -> Number of data points reduced from \",\n",
    "                    X.shape[0],\n",
    "                    \" to \",\n",
    "                    X[index].shape[0],\n",
    "                )\n",
    "                most_common = np.argmax(counts)\n",
    "                print(\n",
    "                    \"Most common duplicate is\",\n",
    "                    index[most_common],\n",
    "                    \" with a count of \",\n",
    "                    counts[most_common],\n",
    "                )\n",
    "        # If we aren't asking for unique use the full index.\n",
    "        # This will save special cases later.\n",
    "        else:\n",
    "            index = list(range(X.shape[0]))\n",
    "            inverse = list(range(X.shape[0]))\n",
    "\n",
    "        # Error check n_neighbors based on data size\n",
    "        if X[index].shape[0] <= self.n_neighbors:\n",
    "            if X[index].shape[0] == 1:\n",
    "                self.embedding_ = np.zeros(\n",
    "                    (1, self.n_components)\n",
    "                )  # needed to sklearn comparability\n",
    "                return self\n",
    "\n",
    "            warn(\n",
    "                \"n_neighbors is larger than the dataset size; truncating to \"\n",
    "                \"X.shape[0] - 1\"\n",
    "            )\n",
    "            self._n_neighbors = X[index].shape[0] - 1\n",
    "        else:\n",
    "            self._n_neighbors = self.n_neighbors\n",
    "\n",
    "        # Note: unless it causes issues for setting 'index', could move this to\n",
    "        # initial sparsity check above\n",
    "        if self._sparse_data and not X.has_sorted_indices:\n",
    "            X.sort_indices()\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Construct fuzzy simplicial set\")\n",
    "\n",
    "        # Handle small cases efficiently by computing all distances\n",
    "        if X[index].shape[0] < 4096 and not self.force_approximation_algorithm:\n",
    "            self._small_data = True\n",
    "            try:\n",
    "                # sklearn pairwise_distances fails for callable metric on sparse data\n",
    "                _m = self.metric if self._sparse_data else self._input_distance_func\n",
    "                dmat = pairwise_distances(X[index], metric=_m, **self._metric_kwds)\n",
    "            except (ValueError, TypeError) as e:\n",
    "                # metric is numba.jit'd or not supported by sklearn,\n",
    "                # fallback to pairwise special\n",
    "                if self._sparse_data:\n",
    "                    dmat = dist.pairwise_special_metric(\n",
    "                        X[index].toarray(),\n",
    "                        metric=self._input_distance_func,\n",
    "                        kwds=self._metric_kwds,\n",
    "                    )\n",
    "                else:\n",
    "                    dmat = dist.pairwise_special_metric(\n",
    "                        X[index],\n",
    "                        metric=self._input_distance_func,\n",
    "                        kwds=self._metric_kwds,\n",
    "                    )\n",
    "            self.graph_, self._sigmas, self._rhos = fuzzy_simplicial_set(\n",
    "                dmat,\n",
    "                self._n_neighbors,\n",
    "                random_state,\n",
    "                \"precomputed\",\n",
    "                self._metric_kwds,\n",
    "                None,\n",
    "                None,\n",
    "                self.angular_rp_forest,\n",
    "                self.set_op_mix_ratio,\n",
    "                self.local_connectivity,\n",
    "                True,\n",
    "                self.verbose,\n",
    "            )\n",
    "        else:\n",
    "            # Standard case\n",
    "            self._small_data = False\n",
    "            # pass string identifier if pynndescent also defines distance metric\n",
    "            if _HAVE_PYNNDESCENT:\n",
    "                if self._sparse_data and self.metric in pynn_sparse_named_distances:\n",
    "                    nn_metric = self.metric\n",
    "                elif not self._sparse_data and self.metric in pynn_named_distances:\n",
    "                    nn_metric = self.metric\n",
    "                else:\n",
    "                    nn_metric = self._input_distance_func\n",
    "            else:\n",
    "                nn_metric = self._input_distance_func\n",
    "            (self._knn_indices, self._knn_dists, self._rp_forest) = nearest_neighbors(\n",
    "                X[index],\n",
    "                self._n_neighbors,\n",
    "                nn_metric,\n",
    "                self._metric_kwds,\n",
    "                self.angular_rp_forest,\n",
    "                random_state,\n",
    "                self.low_memory,\n",
    "                use_pynndescent=True,\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "\n",
    "            self.graph_, self._sigmas, self._rhos = fuzzy_simplicial_set(\n",
    "                X[index],\n",
    "                self.n_neighbors,\n",
    "                random_state,\n",
    "                nn_metric,\n",
    "                self._metric_kwds,\n",
    "                self._knn_indices,\n",
    "                self._knn_dists,\n",
    "                self.angular_rp_forest,\n",
    "                self.set_op_mix_ratio,\n",
    "                self.local_connectivity,\n",
    "                True,\n",
    "                self.verbose,\n",
    "            )\n",
    "\n",
    "            if not _HAVE_PYNNDESCENT:\n",
    "                self._search_graph = scipy.sparse.lil_matrix(\n",
    "                    (X[index].shape[0], X[index].shape[0]), dtype=np.int8\n",
    "                )\n",
    "                _rows = []\n",
    "                _data = []\n",
    "                for i in self._knn_indices:\n",
    "                    _non_neg = i[i >= 0]\n",
    "                    _rows.append(_non_neg)\n",
    "                    _data.append(np.ones(_non_neg.shape[0], dtype=np.int8))\n",
    "\n",
    "                self._search_graph.rows = _rows\n",
    "                self._search_graph.data = _data\n",
    "                self._search_graph = self._search_graph.maximum(\n",
    "                    self._search_graph.transpose()\n",
    "                ).tocsr()\n",
    "\n",
    "                if (self.metric != \"precomputed\") and (len(self._metric_kwds) > 0):\n",
    "                    # Create a partial function for distances with arguments\n",
    "                    _distance_func = self._input_distance_func\n",
    "                    _dist_args = tuple(self._metric_kwds.values())\n",
    "                    if self._sparse_data:\n",
    "\n",
    "                        @numba.njit()\n",
    "                        def _partial_dist_func(ind1, data1, ind2, data2):\n",
    "                            return _distance_func(ind1, data1, ind2, data2, *_dist_args)\n",
    "\n",
    "                        self._input_distance_func = _partial_dist_func\n",
    "                    else:\n",
    "\n",
    "                        @numba.njit()\n",
    "                        def _partial_dist_func(x, y):\n",
    "                            return _distance_func(x, y, *_dist_args)\n",
    "\n",
    "                        self._input_distance_func = _partial_dist_func\n",
    "\n",
    "        # Currently not checking if any duplicate points have differing labels\n",
    "        # Might be worth throwing a warning...\n",
    "        if y is not None:\n",
    "            len_X = len(X) if not self._sparse_data else X.shape[0]\n",
    "            if len_X != len(y):\n",
    "                raise ValueError(\n",
    "                    \"Length of x = {len_x}, length of y = {len_y}, while it must be equal.\".format(\n",
    "                        len_x=len_X, len_y=len(y)\n",
    "                    )\n",
    "                )\n",
    "            y_ = check_array(y, ensure_2d=False)[index]\n",
    "            if self.target_metric == \"categorical\":\n",
    "                if self.target_weight < 1.0:\n",
    "                    far_dist = 2.5 * (1.0 / (1.0 - self.target_weight))\n",
    "                else:\n",
    "                    far_dist = 1.0e12\n",
    "                self.graph_ = discrete_metric_simplicial_set_intersection(\n",
    "                    self.graph_, y_, far_dist=far_dist\n",
    "                )\n",
    "            elif self.target_metric in dist.DISCRETE_METRICS:\n",
    "                if self.target_weight < 1.0:\n",
    "                    scale = 2.5 * (1.0 / (1.0 - self.target_weight))\n",
    "                else:\n",
    "                    scale = 1.0e12\n",
    "                # self.graph_ = discrete_metric_simplicial_set_intersection(\n",
    "                #     self.graph_,\n",
    "                #     y_,\n",
    "                #     metric=self.target_metric,\n",
    "                #     metric_kws=self.target_metric_kwds,\n",
    "                #     metric_scale=scale\n",
    "                # )\n",
    "\n",
    "                metric_kws = dist.get_discrete_params(y_, self.target_metric)\n",
    "\n",
    "                self.graph_ = discrete_metric_simplicial_set_intersection(\n",
    "                    self.graph_,\n",
    "                    y_,\n",
    "                    metric=self.target_metric,\n",
    "                    metric_kws=metric_kws,\n",
    "                    metric_scale=scale,\n",
    "                )\n",
    "            else:\n",
    "                if self.target_n_neighbors == -1:\n",
    "                    target_n_neighbors = self._n_neighbors\n",
    "                else:\n",
    "                    target_n_neighbors = self.target_n_neighbors\n",
    "\n",
    "                # Handle the small case as precomputed as before\n",
    "                if y.shape[0] < 4096:\n",
    "                    try:\n",
    "                        ydmat = pairwise_distances(\n",
    "                            y_[np.newaxis, :].T,\n",
    "                            metric=self.target_metric,\n",
    "                            **self._target_metric_kwds\n",
    "                        )\n",
    "                    except (TypeError, ValueError):\n",
    "                        ydmat = dist.pairwise_special_metric(\n",
    "                            y_[np.newaxis, :].T,\n",
    "                            metric=self.target_metric,\n",
    "                            kwds=self._target_metric_kwds,\n",
    "                        )\n",
    "\n",
    "                    target_graph, target_sigmas, target_rhos = fuzzy_simplicial_set(\n",
    "                        ydmat,\n",
    "                        target_n_neighbors,\n",
    "                        random_state,\n",
    "                        \"precomputed\",\n",
    "                        self._target_metric_kwds,\n",
    "                        None,\n",
    "                        None,\n",
    "                        False,\n",
    "                        1.0,\n",
    "                        1.0,\n",
    "                        False,\n",
    "                    )\n",
    "                else:\n",
    "                    # Standard case\n",
    "                    target_graph, target_sigmas, target_rhos = fuzzy_simplicial_set(\n",
    "                        y_[np.newaxis, :].T,\n",
    "                        target_n_neighbors,\n",
    "                        random_state,\n",
    "                        self.target_metric,\n",
    "                        self._target_metric_kwds,\n",
    "                        None,\n",
    "                        None,\n",
    "                        False,\n",
    "                        1.0,\n",
    "                        1.0,\n",
    "                        False,\n",
    "                    )\n",
    "                # product = self.graph_.multiply(target_graph)\n",
    "                # # self.graph_ = 0.99 * product + 0.01 * (self.graph_ +\n",
    "                # #                                        target_graph -\n",
    "                # #                                        product)\n",
    "                # self.graph_ = product\n",
    "                self.graph_ = general_simplicial_set_intersection(\n",
    "                    self.graph_, target_graph, self.target_weight\n",
    "                )\n",
    "                self.graph_ = reset_local_connectivity(self.graph_)\n",
    "\n",
    "        if self.n_epochs is None:\n",
    "            n_epochs = 0\n",
    "        else:\n",
    "            n_epochs = self.n_epochs\n",
    "\n",
    "        if self.verbose:\n",
    "            print(ts(), \"Construct embedding\")\n",
    "\n",
    "        self.embedding_ = simplicial_set_embedding(\n",
    "            self._raw_data[index],  # JH why raw data?\n",
    "            self.graph_,\n",
    "            self.n_components,\n",
    "            self._initial_alpha,\n",
    "            self._a,\n",
    "            self._b,\n",
    "            self.repulsion_strength,\n",
    "            self.negative_sample_rate,\n",
    "            n_epochs,\n",
    "            init,\n",
    "            random_state,\n",
    "            self._input_distance_func,\n",
    "            self._metric_kwds,\n",
    "            self._output_distance_func,\n",
    "            self._output_metric_kwds,\n",
    "            self.output_metric in (\"euclidean\", \"l2\"),\n",
    "            self.random_state is None,\n",
    "            self.verbose,\n",
    "        )[inverse]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(ts() + \" Finished embedding\")\n",
    "\n",
    "        self._input_hash = joblib.hash(self._raw_data)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit X into an embedded space and return that transformed\n",
    "        output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "            If the metric is 'precomputed' X must be a square distance\n",
    "            matrix. Otherwise it contains a sample per row.\n",
    "        y : array, shape (n_samples)\n",
    "            A target array for supervised dimension reduction. How this is\n",
    "            handled is determined by parameters UMAP was instantiated with.\n",
    "            The relevant attributes are ``target_metric`` and\n",
    "            ``target_metric_kwds``.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_components)\n",
    "            Embedding of the training data in low-dimensional space.\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.embedding_\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X into the existing embedded space and return that\n",
    "        transformed output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            New data to be transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_components)\n",
    "            Embedding of the new data in low-dimensional space.\n",
    "        \"\"\"\n",
    "        # If we fit just a single instance then error\n",
    "        if self.embedding_.shape[0] == 1:\n",
    "            raise ValueError(\n",
    "                \"Transform unavailable when model was fit with only a single data sample.\"\n",
    "            )\n",
    "        # If we just have the original input then short circuit things\n",
    "        X = check_array(X, dtype=np.float32, accept_sparse=\"csr\", order=\"C\")\n",
    "        x_hash = joblib.hash(X)\n",
    "        if x_hash == self._input_hash:\n",
    "            return self.embedding_\n",
    "\n",
    "        if self.metric == \"precomputed\":\n",
    "            raise ValueError(\n",
    "                \"Transform  of new data not available for precomputed metric.\"\n",
    "            )\n",
    "\n",
    "        # X = check_array(X, dtype=np.float32, order=\"C\", accept_sparse=\"csr\")\n",
    "        random_state = check_random_state(self.transform_seed)\n",
    "        rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n",
    "\n",
    "        if self._small_data:\n",
    "            try:\n",
    "                # sklearn pairwise_distances fails for callable metric on sparse data\n",
    "                _m = self.metric if self._sparse_data else self._input_distance_func\n",
    "                dmat = pairwise_distances(\n",
    "                    X, self._raw_data, metric=_m, **self._metric_kwds\n",
    "                )\n",
    "            except (TypeError, ValueError):\n",
    "                dmat = dist.pairwise_special_metric(\n",
    "                    X,\n",
    "                    self._raw_data,\n",
    "                    metric=self._input_distance_func,\n",
    "                    kwds=self._metric_kwds,\n",
    "                )\n",
    "            indices = np.argpartition(dmat, self._n_neighbors)[:, : self._n_neighbors]\n",
    "            dmat_shortened = submatrix(dmat, indices, self._n_neighbors)\n",
    "            indices_sorted = np.argsort(dmat_shortened)\n",
    "            indices = submatrix(indices, indices_sorted, self._n_neighbors)\n",
    "            dists = submatrix(dmat_shortened, indices_sorted, self._n_neighbors)\n",
    "        elif _HAVE_PYNNDESCENT:\n",
    "            indices, dists = self._rp_forest.query(X, self.n_neighbors)\n",
    "        elif self._sparse_data:\n",
    "            if not scipy.sparse.issparse(X):\n",
    "                X = scipy.sparse.csr_matrix(X)\n",
    "\n",
    "            init = sparse_nn.sparse_initialise_search(\n",
    "                self._rp_forest,\n",
    "                self._raw_data.indices,\n",
    "                self._raw_data.indptr,\n",
    "                self._raw_data.data,\n",
    "                X.indices,\n",
    "                X.indptr,\n",
    "                X.data,\n",
    "                int(\n",
    "                    self._n_neighbors\n",
    "                    * self.transform_queue_size\n",
    "                    * (1 + int(self._sparse_data))\n",
    "                ),\n",
    "                rng_state,\n",
    "                self._input_distance_func,\n",
    "            )\n",
    "            result = sparse_nn.sparse_initialized_nnd_search(\n",
    "                self._raw_data.indices,\n",
    "                self._raw_data.indptr,\n",
    "                self._raw_data.data,\n",
    "                self._search_graph.indptr,\n",
    "                self._search_graph.indices,\n",
    "                init,\n",
    "                X.indices,\n",
    "                X.indptr,\n",
    "                X.data,\n",
    "                self._input_distance_func,\n",
    "            )\n",
    "\n",
    "            indices, dists = deheap_sort(result)\n",
    "            indices = indices[:, : self._n_neighbors]\n",
    "            dists = dists[:, : self._n_neighbors]\n",
    "        else:\n",
    "            init = initialise_search(\n",
    "                self._rp_forest,\n",
    "                self._raw_data,\n",
    "                X,\n",
    "                int(self._n_neighbors * self.transform_queue_size),\n",
    "                rng_state,\n",
    "                self._input_distance_func,\n",
    "            )\n",
    "            result = initialized_nnd_search(\n",
    "                self._raw_data,\n",
    "                self._search_graph.indptr,\n",
    "                self._search_graph.indices,\n",
    "                init,\n",
    "                X,\n",
    "                self._input_distance_func,\n",
    "            )\n",
    "\n",
    "            indices, dists = deheap_sort(result)\n",
    "            indices = indices[:, : self._n_neighbors]\n",
    "            dists = dists[:, : self._n_neighbors]\n",
    "\n",
    "        dists = dists.astype(np.float32, order=\"C\")\n",
    "\n",
    "        adjusted_local_connectivity = max(0.0, self.local_connectivity - 1.0)\n",
    "        sigmas, rhos = smooth_knn_dist(\n",
    "            dists,\n",
    "            float(self._n_neighbors),\n",
    "            local_connectivity=float(adjusted_local_connectivity),\n",
    "        )\n",
    "\n",
    "        rows, cols, vals = compute_membership_strengths(indices, dists, sigmas, rhos)\n",
    "\n",
    "        graph = scipy.sparse.coo_matrix(\n",
    "            (vals, (rows, cols)), shape=(X.shape[0], self._raw_data.shape[0])\n",
    "        )\n",
    "\n",
    "        # This was a very specially constructed graph with constant degree.\n",
    "        # That lets us do fancy unpacking by reshaping the csr matrix indices\n",
    "        # and data. Doing so relies on the constant degree assumption!\n",
    "        csr_graph = normalize(graph.tocsr(), norm=\"l1\")\n",
    "        inds = csr_graph.indices.reshape(X.shape[0], self._n_neighbors)\n",
    "        weights = csr_graph.data.reshape(X.shape[0], self._n_neighbors)\n",
    "        embedding = init_transform(inds, weights, self.embedding_)\n",
    "\n",
    "        if self.n_epochs is None:\n",
    "            # For smaller datasets we can use more epochs\n",
    "            if graph.shape[0] <= 10000:\n",
    "                n_epochs = 100\n",
    "            else:\n",
    "                n_epochs = 30\n",
    "        else:\n",
    "            n_epochs = int(self.n_epochs // 3.0)\n",
    "\n",
    "        graph.data[graph.data < (graph.data.max() / float(n_epochs))] = 0.0\n",
    "        graph.eliminate_zeros()\n",
    "\n",
    "        epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n",
    "\n",
    "        head = graph.row\n",
    "        tail = graph.col\n",
    "        weight = graph.data\n",
    "\n",
    "        # optimize_layout = make_optimize_layout(\n",
    "        #     self._output_distance_func,\n",
    "        #     tuple(self.output_metric_kwds.values()),\n",
    "        # )\n",
    "\n",
    "        if self.output_metric == \"euclidean\":\n",
    "            embedding = optimize_layout_euclidean(\n",
    "                embedding,\n",
    "                self.embedding_.astype(np.float32, copy=True),  # Fixes #179 & #217,\n",
    "                head,\n",
    "                tail,\n",
    "                n_epochs,\n",
    "                graph.shape[1],\n",
    "                epochs_per_sample,\n",
    "                self._a,\n",
    "                self._b,\n",
    "                rng_state,\n",
    "                self.repulsion_strength,\n",
    "                self._initial_alpha / 4.0,\n",
    "                self.negative_sample_rate,\n",
    "                self.random_state is None,\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        else:\n",
    "            embedding = optimize_layout_generic(\n",
    "                embedding,\n",
    "                self.embedding_.astype(np.float32, copy=True),  # Fixes #179 & #217\n",
    "                head,\n",
    "                tail,\n",
    "                n_epochs,\n",
    "                graph.shape[1],\n",
    "                epochs_per_sample,\n",
    "                self._a,\n",
    "                self._b,\n",
    "                rng_state,\n",
    "                self.repulsion_strength,\n",
    "                self._initial_alpha / 4.0,\n",
    "                self.negative_sample_rate,\n",
    "                self._output_distance_func,\n",
    "                tuple(self._output_metric_kwds.values()),\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "\n",
    "        return embedding\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Transform X in the existing embedded space back into the input\n",
    "        data space and return that transformed output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_components)\n",
    "            New points to be inverse transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_features)\n",
    "            Generated data points new data in data space.\n",
    "        \"\"\"\n",
    "\n",
    "        if self._sparse_data:\n",
    "            raise ValueError(\"Inverse transform not available for sparse input.\")\n",
    "        elif self._inverse_distance_func is None:\n",
    "            raise ValueError(\"Inverse transform not available for given metric.\")\n",
    "\n",
    "        X = check_array(X, dtype=np.float32, order=\"C\")\n",
    "        random_state = check_random_state(self.transform_seed)\n",
    "        rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n",
    "\n",
    "        # build Delaunay complex (Does this not assume a roughly euclidean output metric)?\n",
    "        deltri = scipy.spatial.Delaunay(\n",
    "            self.embedding_, incremental=True, qhull_options=\"QJ\"\n",
    "        )\n",
    "        neighbors = deltri.simplices[deltri.find_simplex(X)]\n",
    "        adjmat = scipy.sparse.lil_matrix(\n",
    "            (self.embedding_.shape[0], self.embedding_.shape[0]), dtype=int\n",
    "        )\n",
    "        for i in np.arange(0, deltri.simplices.shape[0]):\n",
    "            for j in deltri.simplices[i]:\n",
    "                if j < self.embedding_.shape[0]:\n",
    "                    idx = deltri.simplices[i][\n",
    "                        deltri.simplices[i] < self.embedding_.shape[0]\n",
    "                    ]\n",
    "                    adjmat[j, idx] = 1\n",
    "                    adjmat[idx, j] = 1\n",
    "\n",
    "        adjmat = scipy.sparse.csr_matrix(adjmat)\n",
    "\n",
    "        min_vertices = self._raw_data.shape[-1]\n",
    "\n",
    "        neighborhood = [\n",
    "            breadth_first_search(adjmat, v[0], min_vertices=min_vertices)\n",
    "            for v in neighbors\n",
    "        ]\n",
    "        if callable(self.output_metric):\n",
    "            # need to create another numba.jit-able wrapper for callable\n",
    "            # output_metrics that return a tuple (already checked that it does\n",
    "            # during param validation in `fit` method)\n",
    "            _out_m = self.output_metric\n",
    "\n",
    "            @numba.njit(fastmath=True)\n",
    "            def _output_dist_only(x, y, *kwds):\n",
    "                return _out_m(x, y, *kwds)[0]\n",
    "\n",
    "            dist_only_func = _output_dist_only\n",
    "        elif self.output_metric in dist.named_distances.keys():\n",
    "            dist_only_func = dist.named_distances[self.output_metric]\n",
    "        else:\n",
    "            # shouldn't really ever get here because of checks already performed,\n",
    "            # but works as a failsafe in case attr was altered manually after fitting\n",
    "            raise ValueError(\n",
    "                \"Unrecognized output metric: {}\".format(self.output_metric)\n",
    "            )\n",
    "\n",
    "        dist_args = tuple(self._output_metric_kwds.values())\n",
    "        distances = [\n",
    "            np.array(\n",
    "                [\n",
    "                    dist_only_func(X[i], self.embedding_[nb], *dist_args)\n",
    "                    for nb in neighborhood[i]\n",
    "                ]\n",
    "            )\n",
    "            for i in range(X.shape[0])\n",
    "        ]\n",
    "        idx = np.array([np.argsort(e)[:min_vertices] for e in distances])\n",
    "\n",
    "        dists_output_space = np.array(\n",
    "            [distances[i][idx[i]] for i in range(len(distances))]\n",
    "        )\n",
    "        indices = np.array([neighborhood[i][idx[i]] for i in range(len(neighborhood))])\n",
    "\n",
    "        rows, cols, distances = np.array(\n",
    "            [\n",
    "                [i, indices[i, j], dists_output_space[i, j]]\n",
    "                for i in range(indices.shape[0])\n",
    "                for j in range(min_vertices)\n",
    "            ]\n",
    "        ).T\n",
    "\n",
    "        # calculate membership strength of each edge\n",
    "        weights = 1 / (1 + self._a * distances ** (2 * self._b))\n",
    "\n",
    "        # compute 1-skeleton\n",
    "        # convert 1-skeleton into coo_matrix adjacency matrix\n",
    "        graph = scipy.sparse.coo_matrix(\n",
    "            (weights, (rows, cols)), shape=(X.shape[0], self._raw_data.shape[0])\n",
    "        )\n",
    "\n",
    "        # That lets us do fancy unpacking by reshaping the csr matrix indices\n",
    "        # and data. Doing so relies on the constant degree assumption!\n",
    "        # csr_graph = graph.tocsr()\n",
    "        csr_graph = normalize(graph.tocsr(), norm=\"l1\")\n",
    "        inds = csr_graph.indices.reshape(X.shape[0], min_vertices)\n",
    "        weights = csr_graph.data.reshape(X.shape[0], min_vertices)\n",
    "        inv_transformed_points = init_transform(inds, weights, self._raw_data)\n",
    "\n",
    "        if self.n_epochs is None:\n",
    "            # For smaller datasets we can use more epochs\n",
    "            if graph.shape[0] <= 10000:\n",
    "                n_epochs = 100\n",
    "            else:\n",
    "                n_epochs = 30\n",
    "        else:\n",
    "            n_epochs = int(self.n_epochs // 3.0)\n",
    "\n",
    "        # graph.data[graph.data < (graph.data.max() / float(n_epochs))] = 0.0\n",
    "        # graph.eliminate_zeros()\n",
    "\n",
    "        epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n",
    "\n",
    "        head = graph.row\n",
    "        tail = graph.col\n",
    "        weight = graph.data\n",
    "\n",
    "        inv_transformed_points = optimize_layout_inverse(\n",
    "            inv_transformed_points,\n",
    "            self._raw_data,\n",
    "            head,\n",
    "            tail,\n",
    "            weight,\n",
    "            self._sigmas,\n",
    "            self._rhos,\n",
    "            n_epochs,\n",
    "            graph.shape[1],\n",
    "            epochs_per_sample,\n",
    "            self._a,\n",
    "            self._b,\n",
    "            rng_state,\n",
    "            self.repulsion_strength,\n",
    "            self._initial_alpha / 4.0,\n",
    "            self.negative_sample_rate,\n",
    "            self._inverse_distance_func,\n",
    "            tuple(self._metric_kwds.values()),\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "\n",
    "        return inv_transformed_points\n",
    "\n",
    "\n",
    "class DataFrameUMAP(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metrics,\n",
    "        n_neighbors=15,\n",
    "        n_components=2,\n",
    "        output_metric=\"euclidean\",\n",
    "        output_metric_kwds=None,\n",
    "        n_epochs=None,\n",
    "        learning_rate=1.0,\n",
    "        init=\"spectral\",\n",
    "        min_dist=0.1,\n",
    "        spread=1.0,\n",
    "        set_op_mix_ratio=1.0,\n",
    "        local_connectivity=1.0,\n",
    "        repulsion_strength=1.0,\n",
    "        negative_sample_rate=5,\n",
    "        transform_queue_size=4.0,\n",
    "        a=None,\n",
    "        b=None,\n",
    "        random_state=None,\n",
    "        angular_rp_forest=False,\n",
    "        target_n_neighbors=-1,\n",
    "        target_metric=\"categorical\",\n",
    "        target_metric_kwds=None,\n",
    "        target_weight=0.5,\n",
    "        transform_seed=42,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.metrics = metrics\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.output_metric = output_metric\n",
    "        self.output_metric_kwds = output_metric_kwds\n",
    "        self.n_epochs = n_epochs\n",
    "        self.init = init\n",
    "        self.n_components = n_components\n",
    "        self.repulsion_strength = repulsion_strength\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.spread = spread\n",
    "        self.min_dist = min_dist\n",
    "        self.set_op_mix_ratio = set_op_mix_ratio\n",
    "        self.local_connectivity = local_connectivity\n",
    "        self.negative_sample_rate = negative_sample_rate\n",
    "        self.random_state = random_state\n",
    "        self.angular_rp_forest = angular_rp_forest\n",
    "        self.transform_queue_size = transform_queue_size\n",
    "        self.target_n_neighbors = target_n_neighbors\n",
    "        self.target_metric = target_metric\n",
    "        self.target_metric_kwds = target_metric_kwds\n",
    "        self.target_weight = target_weight\n",
    "        self.transform_seed = transform_seed\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def _validate_parameters(self):\n",
    "        if self.set_op_mix_ratio < 0.0 or self.set_op_mix_ratio > 1.0:\n",
    "            raise ValueError(\"set_op_mix_ratio must be between 0.0 and 1.0\")\n",
    "        if self.repulsion_strength < 0.0:\n",
    "            raise ValueError(\"repulsion_strength cannot be negative\")\n",
    "        if self.min_dist > self.spread:\n",
    "            raise ValueError(\"min_dist must be less than or equal to spread\")\n",
    "        if self.min_dist < 0.0:\n",
    "            raise ValueError(\"min_dist must be greater than 0.0\")\n",
    "        if not isinstance(self.init, str) and not isinstance(self.init, np.ndarray):\n",
    "            raise ValueError(\"init must be a string or ndarray\")\n",
    "        if isinstance(self.init, str) and self.init not in (\"spectral\", \"random\"):\n",
    "            raise ValueError('string init values must be \"spectral\" or \"random\"')\n",
    "        if (\n",
    "            isinstance(self.init, np.ndarray)\n",
    "            and self.init.shape[1] != self.n_components\n",
    "        ):\n",
    "            raise ValueError(\"init ndarray must match n_components value\")\n",
    "        if self.negative_sample_rate < 0:\n",
    "            raise ValueError(\"negative sample rate must be positive\")\n",
    "        if self.learning_rate < 0.0:\n",
    "            raise ValueError(\"learning_rate must be positive\")\n",
    "        if self.n_neighbors < 2:\n",
    "            raise ValueError(\"n_neighbors must be greater than 2\")\n",
    "        if self.target_n_neighbors < 2 and self.target_n_neighbors != -1:\n",
    "            raise ValueError(\"target_n_neighbors must be greater than 2\")\n",
    "        if not isinstance(self.n_components, int):\n",
    "            raise ValueError(\"n_components must be an int\")\n",
    "        if self.n_components < 1:\n",
    "            raise ValueError(\"n_components must be greater than 0\")\n",
    "        if self.n_epochs is not None and (\n",
    "            self.n_epochs <= 10 or not isinstance(self.n_epochs, int)\n",
    "        ):\n",
    "            raise ValueError(\"n_epochs must be a positive integer \" \"larger than 10\")\n",
    "        if self.output_metric_kwds is None:\n",
    "            self._output_metric_kwds = {}\n",
    "        else:\n",
    "            self._output_metric_kwds = self.output_metric_kwds\n",
    "\n",
    "        if callable(self.output_metric):\n",
    "            self._output_distance_func = self.output_metric\n",
    "        elif (\n",
    "            self.output_metric in dist.named_distances\n",
    "            and self.output_metric in dist.named_distances_with_gradients\n",
    "        ):\n",
    "            self._output_distance_func = dist.named_distances_with_gradients[\n",
    "                self.output_metric\n",
    "            ]\n",
    "        elif self.output_metric == \"precomputed\":\n",
    "            raise ValueError(\"output_metric cannnot be 'precomputed'\")\n",
    "        else:\n",
    "            if self.output_metric in dist.named_distances:\n",
    "                raise ValueError(\n",
    "                    \"gradient function is not yet implemented for \"\n",
    "                    + repr(self.output_metric)\n",
    "                    + \".\"\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"output_metric is neither callable, \" + \"nor a recognised string\"\n",
    "                )\n",
    "\n",
    "        # validate metrics argument\n",
    "        assert isinstance(self.metrics, list) or self.metrics == \"infer\"\n",
    "        if self.metrics != \"infer\":\n",
    "            for item in self.metrics:\n",
    "                assert isinstance(item, tuple) and len(item) == 3\n",
    "                assert isinstance(item[0], str)\n",
    "                assert item[1] in dist.named_distances\n",
    "                assert isinstance(item[2], list) and len(item[2]) >= 1\n",
    "\n",
    "                for col in item[2]:\n",
    "                    assert isinstance(col, str) or isinstance(col, int)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self._validate_parameters()\n",
    "\n",
    "        # X should be a pandas dataframe, or np.array; check\n",
    "        # how column transformer handles this.\n",
    "        self._raw_data = X\n",
    "\n",
    "        # Handle all the optional arguments, setting default\n",
    "        if self.a is None or self.b is None:\n",
    "            self._a, self._b = find_ab_params(self.spread, self.min_dist)\n",
    "        else:\n",
    "            self._a = self.a\n",
    "            self._b = self.b\n",
    "\n",
    "        if isinstance(self.init, np.ndarray):\n",
    "            init = check_array(self.init, dtype=np.float32, accept_sparse=False)\n",
    "        else:\n",
    "            init = self.init\n",
    "\n",
    "        self._initial_alpha = self.learning_rate\n",
    "\n",
    "        # Error check n_neighbors based on data size\n",
    "        if X.shape[0] <= self.n_neighbors:\n",
    "            if X.shape[0] == 1:\n",
    "                self.embedding_ = np.zeros(\n",
    "                    (1, self.n_components)\n",
    "                )  # needed to sklearn comparability\n",
    "                return self\n",
    "\n",
    "            warn(\n",
    "                \"n_neighbors is larger than the dataset size; truncating to \"\n",
    "                \"X.shape[0] - 1\"\n",
    "            )\n",
    "            self._n_neighbors = X.shape[0] - 1\n",
    "        else:\n",
    "            self._n_neighbors = self.n_neighbors\n",
    "\n",
    "        if self.metrics == \"infer\":\n",
    "            raise NotImplementedError(\"Metric inference not implemented yet\")\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        self.metric_graphs_ = {}\n",
    "        self._sigmas = {}\n",
    "        self._rhos = {}\n",
    "        self._knn_indices = {}\n",
    "        self._knn_dists = {}\n",
    "        self._rp_forest = {}\n",
    "        self.graph_ = None\n",
    "\n",
    "        def is_discrete_metric(metric_data):\n",
    "            return metric_data[1] in dist.DISCRETE_METRICS\n",
    "\n",
    "        for metric_data in sorted(self.metrics, key=is_discrete_metric):\n",
    "            name, metric, columns = metric_data\n",
    "            print(name, metric, columns)\n",
    "\n",
    "            if metric in dist.DISCRETE_METRICS:\n",
    "                self.metric_graphs_[name] = None\n",
    "                for col in columns:\n",
    "\n",
    "                    discrete_space = X[col].values\n",
    "                    metric_kws = dist.get_discrete_params(discrete_space, metric)\n",
    "\n",
    "                    self.graph_ = discrete_metric_simplicial_set_intersection(\n",
    "                        self.graph_,\n",
    "                        discrete_space,\n",
    "                        metric=metric,\n",
    "                        metric_kws=metric_kws,\n",
    "                    )\n",
    "            else:\n",
    "                # Sparse not supported yet\n",
    "                sub_data = check_array(\n",
    "                    X[columns], dtype=np.float32, accept_sparse=False\n",
    "                )\n",
    "\n",
    "                if X.shape[0] < 4096:\n",
    "                    # small case\n",
    "                    self._small_data = True\n",
    "                    # TODO: metric keywords not supported yet!\n",
    "                    if metric in (\"ll_dirichlet\", \"hellinger\"):\n",
    "                        dmat = dist.pairwise_special_metric(sub_data, metric=metric)\n",
    "                    else:\n",
    "                        dmat = pairwise_distances(sub_data, metric=metric)\n",
    "\n",
    "                    (\n",
    "                        self.metric_graphs_[name],\n",
    "                        self._sigmas[name],\n",
    "                        self._rhos[name],\n",
    "                    ) = fuzzy_simplicial_set(\n",
    "                        dmat,\n",
    "                        self._n_neighbors,\n",
    "                        random_state,\n",
    "                        \"precomputed\",\n",
    "                        {},\n",
    "                        None,\n",
    "                        None,\n",
    "                        self.angular_rp_forest,\n",
    "                        self.set_op_mix_ratio,\n",
    "                        self.local_connectivity,\n",
    "                        False,\n",
    "                        self.verbose,\n",
    "                    )\n",
    "                else:\n",
    "                    self._small_data = False\n",
    "                    # Standard case\n",
    "                    # TODO: metric keywords not supported yet!\n",
    "                    (\n",
    "                        self._knn_indices[name],\n",
    "                        self._knn_dists[name],\n",
    "                        self._rp_forest[name],\n",
    "                    ) = nearest_neighbors(\n",
    "                        sub_data,\n",
    "                        self._n_neighbors,\n",
    "                        metric,\n",
    "                        {},\n",
    "                        self.angular_rp_forest,\n",
    "                        random_state,\n",
    "                        use_pynndescent=True,\n",
    "                        verbose=self.verbose,\n",
    "                    )\n",
    "\n",
    "                    (\n",
    "                        self.metric_graphs_[name],\n",
    "                        self._sigmas[name],\n",
    "                        self._rhos[name],\n",
    "                    ) = fuzzy_simplicial_set(\n",
    "                        sub_data,\n",
    "                        self.n_neighbors,\n",
    "                        random_state,\n",
    "                        metric,\n",
    "                        {},\n",
    "                        self._knn_indices[name],\n",
    "                        self._knn_dists[name],\n",
    "                        self.angular_rp_forest,\n",
    "                        self.set_op_mix_ratio,\n",
    "                        self.local_connectivity,\n",
    "                        False,\n",
    "                        self.verbose,\n",
    "                    )\n",
    "                    # TODO: set up transform data\n",
    "\n",
    "                if self.graph_ is None:\n",
    "                    self.graph_ = self.metric_graphs_[name]\n",
    "                else:\n",
    "                    self.graph_ = general_simplicial_set_intersection(\n",
    "                        self.graph_, self.metric_graphs_[name], 0.5\n",
    "                    )\n",
    "\n",
    "            print(self.graph_.data)\n",
    "            self.graph_ = reset_local_connectivity(\n",
    "                self.graph_, reset_local_metrics=True\n",
    "            )\n",
    "\n",
    "        if self.n_epochs is None:\n",
    "            n_epochs = 0\n",
    "        else:\n",
    "            n_epochs = self.n_epochs\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Construct embedding\")\n",
    "\n",
    "        # TODO: Handle connected component issues properly\n",
    "        # For now we just use manhattan and hope.\n",
    "        self.embedding_ = simplicial_set_embedding(\n",
    "            self._raw_data,\n",
    "            self.graph_,\n",
    "            self.n_components,\n",
    "            self._initial_alpha,\n",
    "            self._a,\n",
    "            self._b,\n",
    "            self.repulsion_strength,\n",
    "            self.negative_sample_rate,\n",
    "            n_epochs,\n",
    "            init,\n",
    "            random_state,\n",
    "            \"manhattan\",\n",
    "            {},\n",
    "            self._output_distance_func,\n",
    "            self.output_metric_kwds,\n",
    "            self.output_metric in (\"euclidean\", \"l2\"),\n",
    "            self.random_state is None,\n",
    "            self.verbose,\n",
    "        )\n",
    "\n",
    "        self._input_hash = joblib.hash(self._raw_data)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
